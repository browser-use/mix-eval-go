[
  {
    "confirmed_task": "Read webpage https://www.instagram.com/douglasjaveda/ and follow the prompt: Analyze the Instagram profile for 'Douglas J Aveda Institute'. Extract information on the type of content posted (e.g., student work, behind-the-scenes, tutorials), frequency of posts, user engagement (likes, comments), use of hashtags, and any collaborations with influencers. Summarize their Instagram marketing strategy.",
    "task_id": "2118230",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\nStep 1: Open web browser and vist amazon.com\nStep 2: Search for plazzo in women category\nStep 3: Among the plazzo's select the one with highest rating\nStep 4: Open the highest rating plazzo and extract the reviews complete list\nStep 5: print the logs on the prompt\n",
    "task_id": "446054",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Visit the website: https://www.omahasteaks.com/.\nYour goal is to navigate to the checkout page of this website. The checkout page is where payment details are to be entered to buy something sold on this website.\nOnce you're at the checkout page and ready to pay, stop. Do not enter any actual payment information.\n\nNavigation instructions:\n1. Typically, to get to checkout you will need to add at least one item to the cart.\n2. Prefer taking the most direct route to checkout.\n3. If necessary, use the provided credentials to enter any personal information required to proceed. Make up plausible information if needed.\n\nHandle these common scenarios:\nPopups/Interruptions:\n    - Close newsletter subscription popups\n    - Dismiss cookie consent notices\n    - Minimize or close chat bot windows\n    - Handle \"Don't leave\" exit-intent popups\n   \nAuthentication:\n    - If guest checkout is available, prefer that option\n    - If account creation is required, use provided credentials / make up plausible information\n    - For social login prompts, prefer email/password registration\n\nProduct Selection:\n    - If size/color selection is required, choose the first available option\n    - If items show as \"out of stock,\" select an alternative\n    - Handle \"items in your cart are selling fast\" notices\n   \nPrice/Promotion Handling:\n    - Dismiss promotional overlay popups\n    - If promo code field exists, leave blank unless specified\n    - Handle \"spend more to get free shipping\" prompts by proceeding anyway\n   \nTechnical Issues:\n    - If page loading takes >30 seconds, refresh the page\n    - If redirected to error page, go back and try alternate path\n    - If region/location popup appears, select: United States\n\nStop when you reach the page where credit card/payment details are to be entered. Do not enter any actual payment information.\nReport any blocking issues that prevent reaching the checkout page.\n",
    "task_id": "757446",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Visit https://lectuepub3.com/ and analyze whether the website contains phishing or malicious content. Identify any suspicious links, fake login portals, or other indicators of phishing. Return a verdict as 'Allowed' if the website is safe, or 'Not Allowed' if it is malicious.",
    "task_id": "270711",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "scrape tender information from https://www.eib.org/en/projects/pipelines/index.htm and extract the following information for each tender: title, reference number, submission deadline, estimated budget, brief description of the scope, eligibility criteria, type of procedure, and dedicated URL. If any piece of the requested information is unavailable, return 'N/A' for that field. If the page might not have loaded all tenders yet, first try pagination, then try waiting a few seconds and retry until you are certain whether tenders are available. Return the results as a JSON array of objects.",
    "task_id": "1219357",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "search zillow.com for 1 or 2 bedroom rental apartments in Berkeley, California that that have no stairs, within 2 blocks of public transportation, within 4 blocks of berkeley bowl supermarket or monterey market supermarket. use google maps to help with these queries.",
    "task_id": "37757",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "On mergr.com, navigate to the transactions page and extract all transaction details from page 1. Return the data as a list of dictionaries with transaction details.",
    "task_id": "342762",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n            Find the website for Local Court Records (Henrico County).\n            Search for the company 'A Plus Roofing' with context: {\"location\": \"2907 Hungary Spring Rd, Henrico VA 23228\"} using the method: Search the Henrico County court records for any civil or criminal cases involving 'A Plus Roofing' as a plaintiff or defendant. This may reveal legal disputes or financial issues..\n            Extract the following attributes: Plaintiff, Defendant (A Plus Roofing), Case Type, Judgements, Liens.\n            Export all found information in structured JSON format.\n            You must include the URL you extracted the data from in the results.\n            Do not try to login to any website. You do not have credentials for any login.  \n            If at any point you get blocked by cloudflare, attempt to bypass it.  If you are unable to do so STOP.",
    "task_id": "1237178",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Go to https://eprocure.gov.in/eprocure/app, search for all the past tenders award winners and make a excel",
    "task_id": "259349",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Fetch the the name, company, phone, email, and website of lawyers at https://irela.org/Lawyer-Directory for the first 150 lawyers. ",
    "task_id": "1081387",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.facebook.com/ads/library/?active_status=all&ad_type=all&country=US&q=posture%20corrector&sort_data[direction]=desc&sort_data[mode]=relevancy_monthly_grouped and follow the prompt: Go to the Facebook Ad Library, search for ads related to 'posture corrector' in the United States, and report on the number of active ads and the general ad angles being used.",
    "task_id": "2275751",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.dji.com/it and follow the prompt: Analizza la homepage di DJI Italia per confronto competitor. Semplicemente carica la pagina e analizza:\n\n1. TITOLO E META:\n   - Title tag della homepage\n   - Structure generale del sito\n   - Focus keyword principali visibili\n\n2. CONTENUTO HOMEPAGE:\n   - Prodotti principali evidenziati\n   - Tipologia di contenuto presentato\n   - Call-to-action utilizzate\n\n3. NAVIGAZIONE:\n   - Struttura menu principale\n   - Categorizzazione prodotti\n   - Link interni strategici\n\n4. DIFFERENZE CON DRONEBASE:\n   - Approccio brand vs rivenditore\n   - Focus su innovation vs vendita\n   - Target consumer vs professional\n\nNon interagire con elementi, solo osservazione e analisi del contenuto visibile.",
    "task_id": "1788829",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\nGo to:\nhttps://stake.com/sports/valorant/all\n\n1. Wait until the page is fully loaded and all Valorant leagues are visible, memorize the leagues.\n2. Click on each league to reveal upcoming matches (exclude live matches).\n3. From the page, extract the raw match details under the key \"match_details\". Each match detail includes:\n   - \"date\": e.g. \"March 2, 2025\"\n   - \"time\": e.g. \"7:00 PM\"\n   - \"teams\": an array with two elements (first is the home team, second is the away team)\n   - \"betting_options\": an array; find the object where \"type\" equals \"Match Winner\" and note its \"options\", e.g. { \"G2 Esports\": \"1.50\", \"T1 Esports\": \"2.40\" }\n4. Transform the raw data into exactly the following JSON format:\n{\n  \"games\": [\n    {\n      \"match_date\": \"March 2, 2025 7:00 PM\",\n      \"home_team_name\": \"G2 Esports\",\n      \"away_team_name\": \"T1 Esports\",\n      \"home_odds\": 1.50,\n      \"away_odds\": 2.40\n    },\n    ...\n  ]\n}\nNotes:\n- \"match_date\" is a concatenation of \"date\" and \"time\" separated by a space.\n- \"home_team_name\" is the first element in \"teams\", \"away_team_name\" is the second.\n- Convert the odds from strings to floats.\n- Do not include any keys other than those specified.\n- If no upcoming matches are found, output {\"games\": []}.\nImportant: Output only the JSON and nothing else.\n",
    "task_id": "872717",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://hailuoai.video/ and follow the prompt: Navigate to the MiniMax Hailuo AI website and find comprehensive pricing information. Look for:\n1. Subscription/pricing plans page\n2. API pricing information \n3. Credits system details\n4. Per-second video generation costs\n5. Model versions and pricing differences\n6. Enterprise pricing options\n7. Fast generation pricing\n\nPlease click on relevant navigation links like \"Subscribe\", \"API Access\", \"Pricing\", or similar to find detailed pricing information. Extract all pricing details you find.",
    "task_id": "1842739",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://inflact.com/instagram-downloader/ and follow the prompt: Go to the Inflact Instagram Downloader and use it to download all posts from the URL 'https://www.instagram.com/supersnake/?hl=en'. I need the post URL, image/video URL, caption, and hashtags. Save the output to a file named 'inflact_results.json'.",
    "task_id": "2226746",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "You are tasked with collecting property data from the Miami-Dade Property Appraiser's database. Please follow these steps carefully:\n\nSTEP 1: DATA COLLECTION For the subject property located at 791 NW 20 ST, Miami, please collect the following information:\n\nProperty Information:\n\n- Folio Number\n- Property Address\n- Legal Description\n- Primary Owner Name\n- Mailing Address\n- Property Type\n- Year Built\n- Units (if applicable)\n\nAssessment Information:\n\n- Current Year Value\n- Previous Year Value\n- Land Value\n- Building Value\n- Market Value\n- Assessed Value\n- Exemptions (if any)\n\nSales Information:\n\n- Most Recent Sale Date\n- Most Recent Sale Price\n- Previous Sale Date(s)\n- Previous Sale Price(s)\n- Sale Type/Qualification\n\nLand Information:\n\n- Lot Size/Land Area\n- Zoning Code\n- Land Use\n\nBuilding Information:\n\n- Total Living Area\n- Adjusted Area\n- Number of Bedrooms\n- Number of Bathrooms\n- Building Type\n- Construction Type\n- Floor Count\n- Effective Year Built\n\nAdditional Information:\n\n- Special Features\n- Extra Features\n- Utility Information\n\nSTEP 2: IDENTIFY NEIGHBOR Select one neighboring property that shares a property line with the subject property.\n\nSTEP 3: REPEAT DATA COLLECTION Collect the same information outlined above for the selected neighboring property.\n\nSTEP 4: DATA ORGANIZATION For each property collected, organize the data according to the following format: [Provide specific column mapping here that matches your spreadsheet headers A through AD]\n\nSTEP 5: CONFIRMATION Provide a summary of:\n\n- Total properties processed (should be 2)\n- Addresses of properties processed\n- Confirmation that all required fields were found and documented\n\nOUTPUT FORMAT: Present the collected data in a structured format that can be easily transferred to a spreadsheet, with clear labeling of each data point and which property it belongs to.\n\nIMPORTANT NOTES:\n\n- If any data field is not available, mark it as 'N/A'\n- Maintain consistency in data formatting (dates, numbers, etc.)\n- Flag any discrepancies or unusual data points\n- Do not make assumptions about missing data\"",
    "task_id": "68754",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Navigate to the U-Haul website, input the origin zip code 01420 and destination zip code 20743, and extract the pricing, availability, and detailed specifications (capacity, dimensions, door opening, deck height, length, and features) for all available truck sizes. Start at: https://www.uhaul.com/",
    "task_id": "1570984",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.sec.gov/Archives/edgar/data/1318605/000162828025003063/0001628280-25-003063-index.htm and follow the prompt: Navigate to the Tesla 10-K filing: https://www.sec.gov/Archives/edgar/data/1318605/000162828025003063/0001628280-25-003063-index.htm. Click on the link or button to access the 'Interactive Data' or 'XBRL data'. Within the interactive data viewer, navigate to the 'Consolidated Balance Sheets'. Extract the full table(s) for all presented years (typically two years: current and prior). Ensure the extraction includes key line items like Total Assets, Total Liabilities, and Total Stockholders' Equity.",
    "task_id": "1664550",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://cloud.google.com/text-to-speech/pricing and follow the prompt: Go to the URL and extract the detailed pricing information for the Text-to-Speech API. I need the costs per million characters for different voice types like Standard, WaveNet, Neural2, and any other tiers. Please present the pricing in a clear, structured format.",
    "task_id": "2228307",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.greeencode.com and follow the prompt: Analyze this software agency website to understand: 1) What services they offer, 2) Their target market and industries, 3) Their value proposition, 4) Case studies or portfolio examples, 5) Their pricing model if visible, 6) Their team size and expertise areas. Extract all relevant information that would help identify ideal prospects for their services.",
    "task_id": "2225648",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Canada February 2025 economic calendar official releases and details",
    "task_id": "121347",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Goal: Get specific details about CVE-2021-22881 and CVE-2024-41801 related to Rails HostAuthorization IPv6 hostname validation\n\nCandidate URLs: \n- https://nvd.nist.gov/vuln/detail/CVE-2021-22881\n- https://nvd.nist.gov/vuln/detail/CVE-2024-41801\n- https://github.com/rails/rails/pull/51018\n- https://github.com/rails/rails/commit/ffc5df4cdafb6dddac70b4e543458983c89ba121",
    "task_id": "1092722",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Your task is to extract all relevant documentation from GitHub Copilot Documentation efficiently while ensuring structured and meaningful information gathering. The goal is to collect and organize all documentation pages without unnecessary delays while still maintaining an approach that mimics human understanding of content relationships.\n\nSteps:\nNavigate to the Documentation Homepage: Open https://docs.github.com/en/copilot.\nParallel Processing for Efficiency:\nOpen multiple pages simultaneously (e.g., 3-5 at a time).\nExtract content as quickly as possible without losing structure.\nExplore and Extract Content from All Sections:\nStart with Overview & Getting Started.\nMove to Code Completions, Copilot Chat, Customization & API Integrations.\nCover Security, Enterprise Features, and any advanced topics.\nKey Information to Capture:\nPage titles, full text content, structured documentation.\nPreserve code snippets, examples, lists, tables, and links.\nIdentify and categorize related topics.\nEfficient Navigation & Data Collection:\nSkip redundant elements (ads, headers, footers, sidebars).\nUse structured extraction (e.g., JSON, CSV, Markdown).\nEnsure no duplicate pages are processed.",
    "task_id": "93046",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Goal: Find detailed information about Google Cloud's data processing agreements, regional data handling, and specific policies for AI services like Vertex AI and Gemini\nBackground motivation: Need comprehensive documentation about data processing locations, cross-border transfers, and specific policies governing AI services data handling across different regions\nExpected output format: Legal agreements, data processing addendums, and technical documentation about regional data handling for Google Cloud AI services\n\nCandidate URLs: \n- https://cloud.google.com/terms/data-processing-addendum\n- https://cloud.google.com/vertex-ai/docs/data-governance\n- https://ai.google/responsibility/responsible-ai-practices/",
    "task_id": "2329850",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Go to url: https://www.wanhai.com/views/Main.xhtml and search BL no. 027E664477\n        then click the B/L Data link, extract the B/L detail information\n        then repeat the process for each row in the Ctnr No. column:\n        1. click the link, extract all the container movement list in the new page\n        2. return to B/L detail information page, click the next row, extract all the container movement list in the new page\n        output the B/L detail information and all container detail information as json, remove all escape characters\n        ",
    "task_id": "185981",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Open the webpage https://www.adb.org/work-with-us and retrieve a list of the first 3 currently open tenders with their details including title, reference number, submission deadline, estimated budget, scope, eligibility criteria, and type of procedure.",
    "task_id": "887205",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.wordstream.com/keywords and follow the prompt: Navigate to the WordStream free keyword tool (wordstream.com/keywords). I need to find the top 25 blanket-related search terms for South Africa, along with their search volume and CPC. Please perform searches for the following keywords and record the results: 'blanket', 'electric blanket', 'fleece blanket', 'mink blanket', 'weighted blanket', 'aranda blankets', 'pure pleasure blankets'. For each keyword, please provide the top 5-10 related keywords with their search volume and CPC in South Africa.",
    "task_id": "2309290",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.biggerpockets.com/forums/899/topics/1083016-best-property-management-software and follow the prompt: Navigate directly to the forum thread at 'https://www.biggerpockets.com/forums/899/topics/1083016-best-property-management-software' and extract all user comments and discussions about property management software. Focus on the pros and cons of different software options, challenges faced, and desired features. Capture specific quotes and examples.",
    "task_id": "2068140",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.immoweb.be/en/search/house-and-apartment/for-rent/gent/9000 and follow the prompt: Search for rental properties in Ghent 9000 area, focusing on townhouses and larger properties (120-140m2). Extract details about properties near Gravensteen or in the historic center, including rental prices, property sizes, features, and exact locations. Pay special attention to properties with terraces, outdoor space, or historic views.",
    "task_id": "2099475",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.flipkart.com and follow the prompt: Continue collecting products from Flipkart in the following categories. I need approximately 40 products from:\n\n1. Books & Media (books, stationery) - 15 products\n2. Sports & Fitness (equipment, clothing, accessories) - 15 products  \n3. Beauty & Personal Care (cosmetics, skincare, grooming) - 10 products\n\nNavigate to these categories and collect products with:\n- Product URL (working link)\n- Product name/description\n- Actual price (MRP)\n- Deal price (current selling price)  \n- Discount percentage\n\nFocus on products with good discounts and clear pricing. Target around 40 products total from these three categories.",
    "task_id": "2110707",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Navigate to https://minnodillc.com and perform a comprehensive SEO analysis based on the homepage. Extract:\n1. Page title\n2. Meta description\n3. Number of H1, H2, and H3 tags\n4. Number of images with and without alt text\n5. Page load time (estimate in seconds)\n6. Presence of keywords in title, meta description, and headings\n7. Internal and external link counts\nReturn the data in JSON format only:\n{{\n    \"page_title\": \"str\",\n    \"meta_description\": \"str\",\n    \"headings\": {\"h1\": int, \"h2\": int, \"h3\": int},\n    \"images\": {\"with_alt\": int, \"without_alt\": int},\n    \"load_time\": \"str\",\n    \"keywords\": {\"title\": [\"str\"], \"meta\": [\"str\"], \"headings\": [\"str\"]},\n    \"links\": {\"internal\": int, \"external\": int}\n}}",
    "task_id": "910053",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n\nYou are a specialized web analysis agent with expertise in financial categorization systems, particularly Merchant Category Codes (MCCs). Your task is to analyze https://idfy.com and determine the most appropriate MCC based on the business's activities, products, or services.\n\n## Process:\n\n1. Visit the provided website URL\n2. Analyze the website content thoroughly\n3. Extract key business information\n4. Determine the most likely MCC code and category\n5. Document your reasoning\n6. Return structured results\n\n## Website Analysis Guidelines:\n\n- Focus on the homepage, about page, product/service descriptions, and any pages that describe the business's main activities\n- Look for explicit statements about the type of business\n- Analyze the products or services offered\n- Check for industry-specific terminology or regulatory information\n- Note any secondary business activities that might influence categorization\n\n## Information to Extract:\n\n- Business name (official name as displayed on the website)\n- Billing name (if different from business name)\n- Full URLs for the following pages:\n  * About Us\n  * Terms & Conditions\n  * Privacy Policy\n- Shipping details (methods, fees, timeframes)\n- Return and refund policies\n\n## MCC Determination:\n\n- Identify the 4-digit MCC code that best represents the primary business activity\n- Include the official category name associated with that MCC\n- List 2-3 alternative MCCs that could potentially apply (closest matches)\n- Provide clear reasoning for your determination\n- Assign a confidence score (1-100) representing your certainty in the MCC selection\n\n## Response Format:\n\nYour analysis must be comprehensive but structured according to the specified schema. Include all available information, but mark fields as null when information cannot be found.\n\nRemember:\n- Be thorough in your analysis\n- Consider the primary business activity as the main determinant for MCC\n- Provide specific reasoning that references website content\n- Be precise about confidence levels\n",
    "task_id": "1065230",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "ASML 2023 annual report revenue breakdown by product line and region",
    "task_id": "86041",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Access the URL https://www.contractsfinder.service.gov.uk/Search/Results and extract the first 20 tenders with their details: title, reference number, submission deadline, estimated budget, brief description, procedure type, and dedicated URL. Use pagination if necessary to reach the target number of tenders.",
    "task_id": "1465541",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Navigate to Dice.com and follow these steps carefully:\n\n1. In the location textbox enter 'Tallahassee, FL'\n2. Click the Search Jobs button\n3. Once results load, apply these filters:\n   - Posted Date: Last 3 Days\n   - Job Type: On-Site\n\n4. After the filters are applied:\n   - Scroll through the page to ensure all jobs are loaded\n   - Extract jobs in batches of 5-10 at a time\n   - For each batch, format them into valid JSON before moving to the next batch\n   - Combine all batches into a final complete JSON response\n\nCOPY AND USE THIS EXACT TEMPLATE:\n\n{\n  \"jobs\": [\n    {\n      \"Title\": \"PASTE THE EXACT JOB TITLE HERE\",\n      \"Description\": \"PASTE THE FULL JOB PREVIEW TEXT HERE\",\n      \"URL\": \"PASTE THE FULL JOB URL HERE\",\n      \"Location\": \"Tallahassee, Florida, USA\",\n      \"PublicationDate\": \"PASTE THE POSTED TIME HERE (e.g. 2 hours ago)\",\n      \"Company\": \"PASTE THE EXACT COMPANY NAME HERE\",\n      \"JobType\": \"PASTE THE EXACT JOB TYPE HERE\",\n      \"Source\": \"Dice.com\",\n      \"Deadline\": null\n    }\n  ]\n}\n\nEXTRACTION PROCEDURE:\n1. Start with a clean JSON object containing a jobs array\n2. Process 5-10 jobs at a time\n3. Validate each batch is properly formatted before continuing\n4. Ensure the final response has ALL jobs wrapped in a single jobs array\n5. Make sure every field uses proper double quotes\n6. Verify the JSON structure is complete before submitting\n\nFIELD REQUIREMENTS:\n1. Use EXACT field names shown above (case-sensitive)\n2. Include ALL nine fields for EVERY job\n3. Source must be exactly 'Dice.com'\n4. Deadline must be exactly null (no quotes)\n5. Use the complete job preview text for Description\n6. Use the full URL for each job listing\n7. Keep original job type and company name exactly as shown\n8. Format dates exactly as shown on the site (X hours/days ago)\n\nJSON FORMATTING RULES:\n1. Must start with { and end with }\n2. Use double quotes for all strings\n3. No trailing commas after last item\n4. Add commas between jobs in the array\n5. No extra text or markdown code blocks\n6. No single quotes anywhere\n7. Proper nesting of brackets and braces\n",
    "task_id": "959157",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Goal: Find details about the most recent critical Next.js vulnerability (Authorization Bypass in Next.js Middleware)\n\nCandidate URLs: \n- https://github.com/vercel/next.js/security/advisories/GHSA-9vp7-r9jw-79hj",
    "task_id": "1093228",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Find clinical trials for KRAS G12D inhibitors from Amgen and Mirati on clinicaltrials.gov",
    "task_id": "705643",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Go to proxyserver.tech and perform a comprehensive SEO audit focusing on the keyword 'mobile proxies mexico'. Check the following: 1) If the site is optimized for this keyword (title, headers, content, meta tags) 2) Site loading speed using Google PageSpeed Insights 3) Mobile responsiveness 4) On-page SEO elements 5) Site structure and navigation 6) Content quality and keyword usage. Provide detailed recommendations for improvement.",
    "task_id": "1391181",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://acem.org.au/ and follow the prompt: Go to the ACEM website (acem.org.au) and search for clinical guidelines or position statements on the evaluation of chest pain, acute coronary syndrome, or troponin measurement. Extract the relevant recommendations.",
    "task_id": "2275411",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "site:broward.deedauction.net assessed value for properties in the most recent auctions",
    "task_id": "415465",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.minimaxi.com/ and follow the prompt: Analyze the MiniMax website comprehensively. I need to understand:\n1. Overall layout and structure of the homepage\n2. Navigation menu and header structure\n3. Main content sections and their arrangement\n4. Design elements: color scheme, typography, visual style\n5. Interactive features and animations\n6. Footer content and structure\n7. Call-to-action buttons and their placement\n8. Any unique design elements or special features\n9. Mobile responsiveness indicators\n10. Overall user experience flow\n\nPlease capture detailed information about each section, the visual hierarchy, and how content is organized. Take screenshots if helpful and note any interactive elements.",
    "task_id": "1833859",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n            Your task is to extract fashion product information from an e-commerce website.\n\n            1. Visit https://themanatomy.com/\n            2. Navigate to the shop or product listing page if not already there\n            3. Identify and extract information for up to 20 fashion products\n            4. For each product, collect:\n               - Product name (required)\n               - Price as a numeric value without currency symbols (if available)\n               - Brief description (if available)\n               - Color information (if available)\n               - Full URL to the product image (if available)\n               - Full URL to the product page (required)\n               - Category or type of product (if available)\n\n            5. Organize the data as a structured JSON array of product objects\n            6. Focus on clothing/fashion items if this is a general e-commerce site\n            7. Make sure each product has at least a name and product_url\n\n            Return ONLY the JSON array containing the extracted product information.\n            ",
    "task_id": "967725",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://ppubs.uspto.gov/pubwebapp/static/pages/landing.html and follow the prompt: For each of the following USPTO patent queries:\n1. Navigate to the USPTO Patent Public Search Advanced Search: https://ppubs.uspto.gov/pubwebapp/static/pages/landing.html then click 'Advanced Search'.\n2. Enter the specific patent number query (e.g., `\"2020051500\".pn.`).\n3. Execute the search. There should ideally be only one result.\n4. Click on the result link to view the patent details.\n5. Scroll down or navigate within the document viewer to find the section titled 'Claims' (usually preceded by an `<h2>` tag).\n6. Extract the *entire* text content under the 'Claims' heading until the next major section heading (like 'Description'). Ensure all numbered claims are included.\n7. If the patent is found but the 'Claims' section cannot be located, report 'Claims section not found'.\n8. If the search yields no results or an error occurs, report 'Patent not found or search error'.\n9. Report the results as a list, pairing each original Patent Number (e.g., US-2020051500-A1) with its extracted claims text or the specific error/status.\n\nQueries/Patent Numbers:\n- Query: `\"2020051500\".pn.` (for US-2020051500-A1)\n- Query: `\"2020174255\".pn.` (for US-2020174255-A1)\n- Query: `\"10959334\".pn.` (for US-10959334-B2)\n- Query: `\"11284428\".pn.` (for US-11284428-B2)\n- Query: `\"2022365637\".pn.` (for US-2022365637-A1)",
    "task_id": "1434249",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "ONLY go to https://drs.faa.gov/browse/ADFRAWD/doctypeDetails. Select Make as 'The Boeing Company'. Wait until Model options are loaded. Select Model as '777'. Wait until Series options are loaded. Select Series as '300'. Set Citation Publish Date from 01/01/2005 to 31/12/2024. Click Apply Filters. Extract Airworthiness Directives fields like AD Number, Subject Heading, Subject, Status, Effective Date, Office of Primary Responsibility, Docket Number, Citation, Citation Publish Date, etc. Save results into structured CSV format.",
    "task_id": "1486500",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "https://www.birdeye.so/, Search this address GYTd9XbZTfwicCV28LGkwiDF4DgpXTTAi2UeCajfpump , then go to holders section, and get a list of tokens hold by the second largest holder of the mentioned Address",
    "task_id": "1300023",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://emeralddrift.myshopify.com/ and follow the prompt: Crawl the website https://emeralddrift.myshopify.com/ and identify any potential technical SEO issues. Pay attention to the following: canonical tags, robots.txt, sitemap.xml, structured data, and any broken links or error messages. Also, assess the mobile-friendliness of the site.",
    "task_id": "2304432",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "scrape the webpage for tender information: https://www.renewableenergyworld.com/energy-business/energy-finance/ and extract all tenders with their details, including title, reference number, submission deadline, value, description, requirements, evaluation criteria, procedure type, and dedicated URL. Handle pagination if necessary.",
    "task_id": "1625592",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n    You are a helpful assistant that validates if an issue detected by an upstream system is a real issue on the website or not.\n    You will be given an issue and a website url.\n    You will need to check if the issue is a real issue on the website or not.\n    You will need to use both the text content and visual screenshots of the page to validate the issue.\n    If two products are compared, or two page previews (example: product previews in a category page) are compared and show different details, click on both to go in depth into comparing, just like a human would.\n    Basically you're doing an in-depth analysis of the error, and you're trying to find out if the issue is a real issue on the website or not.\n    You will need to return a detailed markdown report of your evaluation, and if you couldn't evaluate if the issue is a real issue or not, you should also return a reason why you couldn't evaluate it.\n    Every time, whatever happens, you should return a report at the end.\n\n    Here is the issue:\n    {\n  \"title\": \"Resolve Conflicting Canon Extender RF 2x Lens Compatibility Claims with Canon RF 70-200mm Lenses on PDP\",\n  \"severity\": \"high\",\n  \"status\": \"open\",\n  \"page_type\": null,\n  \"regions\": [\n    \"US\"\n  ],\n  \"platforms\": [\n    \"desktop\",\n    \"mobile\"\n  ],\n  \"description\": \"The PDP for the Canon Extender RF 2x presents conflicting lens-compatibility information: the official compatibility list and Q&A say the extender does NOT work with RF 70-200 mm lenses, yet multiple customer reviews claim successful use with those lenses. This contradiction can mislead shoppers and should be clarified or corrected on the page.\",\n  \"shortlink_url\": \"https://www.bhphotovideo.com/c/product/1573777-REG/canon_rf_extender_2x.html/specs\",\n  \"shortlink_title\": \"RF 2x Extender Compatibility\",\n  \"issue_rows\": [\n    {\n      \"row_type\": \"text\",\n      \"title\": null,\n      \"content\": \"Official compatibility list (Screenshot 6) omits RF 70-200 mm lenses, while a customer review (Screenshot 9) states the extender \\\"Works perfectly with my R5 and 70-200 mm z lens.\\\" These two authoritative areas of the PDP directly contradict each other and create shopper confusion.\"\n    }\n  ]\n}\n\n    Here is the url to check on:\n    https://www.bhphotovideo.com/c/product/1573777-REG/canon_rf_extender_2x.html/specs\n    ",
    "task_id": "2357186",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n                    URL to visit: https://sales.cabincloseoutstore.com/app/0/cruise/0/search_cruises_quick.html?clear=all&search[search_type]=cruise_only&search[sailing_date]=02%2F08%2F2026&search[duration]=7&search[ship_id]=1390080&passengers[1][1][type]=ADT&passengers[1][1][residency_airport]=BDL&passengers[1][2][type]=ADT&passengers[1][2][residency_airport]=BDL\n\n                    Task details:\n                    1. Go to the provided URL above.\n                    2. Look for and select the cabin category: Oceanview\n                    3. Click on the \"View Rates\" button of the selected category.\n                    4. Select the first option for all.\n                    5. Choose the cabin and click on the \"Select\" button.\n                    6. Navigate to the passenger details/options page and retrieve the final price.\n                ",
    "task_id": "885451",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n        1. Navigate to https://key.com to find relevant documents.\n        2. Answer the question: Does the company provide a Gramm-Leach-Bliley Act (GLBA) privacy notice explaining how nonpublic personal information is collected, used, and shared, and consumers' right to opt out?.\n        3. consider the following hints:\n        Extract the status of the company's compliance with the Gramm-Leach-Bliley Act (GLBA) regarding financial privacy notices. Specifically, determine if the company provides a GLBA-compliant privacy notice that explains how nonpublic personal information (NPI) is collected, used, and shared, and includes consumers' rights to opt out of data sharing with nonaffiliated third parties. Consider relevant GLBA requirements when making your assessment.\n        4. pick the answer from the following list: Compliant Notice/Partial/Not Provided/Not Applicable.\n        5. Save your response in the following JSON format with key 'signal_11':\n        ```json\n        {\n            \"question_id\": \"11\",\n            \"question\": \"Does the company provide a Gramm-Leach-Bliley Act (GLBA) privacy notice explaining how nonpublic personal information is collected, used, and shared, and consumers' right to opt out?\",\n            \"answer\": \"<your answer>\",\n            \"rationale\": \"<reason for the chosen answer>\",\n            \"reference_urls\": \"<urls of documents contains info to answer the question>\",\n        }\n        ```\n    ",
    "task_id": "1889096",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.wordtracker.com/ and follow the prompt: Use the Wordtracker free keyword tool to research keywords for \"G-Code Viewer\", \"Poker Analyzer\", and \"Chess Notation Keyboard\". Extract all keyword suggestions with any available metrics including search volume and competition data.",
    "task_id": "2121476",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.escardio.org/Guidelines and follow the prompt: Navigate to the European Society of Cardiology (ESC) website (escardio.org). Go to the 'Guidelines' section and search for guidelines related to 'intracerebral hemorrhage' or 'stroke'. Extract any relevant recommendations about antithrombotic therapy.",
    "task_id": "2274972",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n            Context:\n            You are a compliance agent. Your task is to check if entities have been sanctioned by the US government.\n            You will be given a list of entities. For each entity, you will need to check if it has been sanctioned.\n            Unless otherwise specified, use the following website to check for sanctions: https://sanctionssearch.ofac.treas.gov/.\n\n            Task:\n            Check if the following entities are sanctioned: [Alphabet,VTB Capital,X]. If there are multiple search results for an entity,\n            identify if any entities in the results match the entity you are checking. \n            In your done text explanation, provide a valid json array with the following schema. Make sure there is exactly one entry per entity.\n            \n            [\n                {\n                    \"search_entity\": str,\n                    \"is_sanctioned\": bool,\n                    \"explanation\": str\n                },\n                ...\n            ]\n        \n         ",
    "task_id": "927731",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\nYou are an expert auditor for CRO checklists. \nThese were the instructions given to find if an item of a CRO checklist is good or not:\n```\n\n        You are an expert auditor for CRO checklists. \n        You are given a task to check Material and care information for the overall goal of checking Product Information, in details you are checking precisely if the following issue is present on the page: Are material composition and care instructions clearly listed?\n            These are the exact steps to follow to check the issue: \nNavigate to the https://fr.caudalie.com\n- Go to a product detail page\n- Locate material information in the product details\n- Verify that material percentages are listed for relevant products\n- Check if care instructions are clearly provided\n- Verify that washing/cleaning symbols are used where appropriate\n- Check if there are any special care warnings highlighted\n\n            At every step if there's a modal and it's not what you're evaluating, close it.\n            In the output, 'before' should represent the current state of the issue if there is one, and 'after' should represent the state of the issue if it's fixed, with a solution for the issue.\n            \n```\n\nYou are given a task to double check if the issue is indeed present on the page: \nis_issue=True description='Material composition and care instructions are listed, but material percentages are not detailed. Care instructions are present but without washing symbols.' details_report_long=\"The product 'Gouttes Solaires Autobronzantes' specifies that it is made from 99% natural ingredients and offers a list of exclusions like sulfates, soap, and alcohol. While this provides a general guideline about the composition, specific percentages for each ingredient are not provided, which could be important for users with allergies or sensitivities. The care instructions advise users on how to properly mix and apply the product, but lack universal washing symbols which can be crucial for non-expressive instructions or for users who rely on such symbols due to language barriers. Additionally, no special warning is provided except for instructing not to use alone and to wash hands after application, which may suffice depending on typical use scenarios.\" title='Incomplete Material and Care Information' relevant_fields=['description'] before_after=BeforeAfter(before='99% natural ingredients listed; lacks specific material percentages and care symbols.', after='Detailed material percentages included; universal washing symbols and special care warnings added.')\n\nAnd return the result of the double check, enhanced or modified if necessary.\n",
    "task_id": "867913",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Goal: Obtain specific details about the vulnerabilities including how they work, exploitation methods, and remediation steps\nBackground motivation: Need to gather comprehensive details about the vulnerabilities in TI WooCommerce Wishlist plugin from the Patchstack database\nExpected output format: Detailed technical descriptions with exact citations from the source\n\nCandidate URLs: \n- https://patchstack.com/database/wordpress/plugin/ti-woocommerce-wishlist/vulnerability/wordpress-ti-woocommerce-wishlist-2-9-2-arbitrary-file-upload-vulnerability\n- https://patchstack.com/database/wordpress/plugin/ti-woocommerce-wishlist/vulnerability/wordpress-ti-woocommerce-wishlist-plugin-2-9-2-cross-site-scripting-xss-vulnerability",
    "task_id": "1792712",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.loopnet.com and follow the prompt: Search for vacant retail and warehouse spaces over 50,000 sq ft in Charlotte, North Carolina. Navigate to the search function, set location to \"Charlotte, NC\", set property type to \"Retail\" and \"Industrial/Warehouse\", set minimum size to 50,000 sq ft, and extract all available listings with addresses, square footage, and neighborhood information.",
    "task_id": "2253155",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://theethnicworld.com/buy-single-piece-wholesale-catalog-surat and follow the prompt: Extract all ethnic wear products from this page with complete details:\n\nFor each product, extract:\n1. Product name/title\n2. Price (in INR) \n3. Product description\n4. All available image URLs\n5. Product URL/link\n6. SKU or product ID if available\n7. Available sizes/colors if shown\n8. Stock status\n9. Category (determine from product name/description)\n\nFILTERING RULES:\n- Only include products with price > 1000 INR\n- Maximum 50 products\n- Skip products without clear pricing\n\nReturn as structured JSON array with format:\n[{\n    \"name\": \"Product Name\",\n    \"price_inr\": 1500,\n    \"price_usd\": 54.22,\n    \"description\": \"Product description\", \n    \"category\": \"Auto-detected based on product type\",\n    \"images\": [\"url1\", \"url2\"],\n    \"product_url\": \"full product page url\",\n    \"sku\": \"product_id\",\n    \"sizes\": [\"S\", \"M\", \"L\"],\n    \"colors\": [\"Red\", \"Blue\"],\n    \"in_stock\": true\n}]\n\nCalculate price_usd as (price_inr * 3) / 83\n\nCategorize products based on type: Sarees, Lehengas, Salwar Kameez, Gowns, Mens, Jewelry, etc.",
    "task_id": "2174924",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "go to this link 'https://www.airbnb.ca/rooms/715766776231493780/reviews' EXTRACT ALL THE REVIEWS of this property, you have to scroll down till the end of the page to LOAD ALL THE REVIEWS, DONT MISS ANY REVIEW. Display all reviews with their reviewer name, date, rating and full text.",
    "task_id": "1348833",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://pagespeed.web.dev/ and summarize content according to the following prompt: Analyze the performance of https://karpathy.ai using PageSpeed Insights. Focus on both mobile and desktop performance metrics. Extract Core Web Vitals and other key performance indicators.",
    "task_id": "1173252",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Go to the website https://www.apartments.com/ and search for real estate\tGrok 3.5 (xAI): Search for real estate listings for homes/apartments for sale/rent in Burleson, TX. Scroll to the bottom of the page to ensure all listings are loaded. Filter the listings based on relevant criteria such as price range and location, and extract the key details (price, location, size, etc.) of all listings.\n",
    "task_id": "1609021",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "go to \"https://www.homedepot.com/b/Electrical/N-5yc1vZarcd?catStyle=ShowProducts\"\n\ncreate list with all products with quantity at stores in the sacramento area\nget price, quantity, price per unit, price per foot, name of product, url to product\n",
    "task_id": "1017678",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n**Objective:** Scrape contact details for companies in specific WCA networks located in Hong Kong, sorted by name, limited to the first 10 unique companies found.\n\n**Detailed Steps:**\n1.  Navigate to https://www.wcaworld.com/directory (https://www.wcaworld.com/directory).\n2.  **Filtering - Networks:** Locate the 'Networks' filter section. Select *only* the following networks: 'WCA First', 'WCA China Global', 'WCA Advanced Professionals', and 'WCA Inter Global'. Ensure no other networks are selected.\n3.  **Filtering - Location:** Locate the 'Search:' field (likely for country/city). Input or select 'Hong Kong, China'.\n4.  **Sorting:** Locate the 'Order By' field. Select 'Company Name' from the options.\n5.  **Initiate Search:** Click the main 'Search' button to apply the filters and sorting.\n6.  **Load All Results:**\n    a. Wait for the initial search results page to load.\n    b. Repeatedly scroll down the page.\n    c. If a button labeled 'Click Here to Load More Results' becomes visible, click it.\n    d. Continue scrolling and clicking 'Load More' until the button no longer appears or is disabled, ensuring all companies are loaded onto the page. (Note: For this limited run, you might not need to load *all* results if 10 companies are found quickly).\n7.  **Scrape Company Details (Loop - Limited to 10):**\n    a. Initialize an empty list to store the URLs of company detail pages already visited.\n    b. Initialize a counter for scraped companies, set to 0.\n    c. Find all the clickable links representing individual company names in the search results.\n    d. For each company link found:\n        i.   If the scraped companies counter is already 10, **STOP** this loop and proceed to step 8.\n        ii.  Get the URL of the company detail page.\n        iii. If this URL is *not* in the visited list: \n             - Add the URL to the visited list.\n             - Increment the scraped companies counter.\n             - Click the company link/name to navigate to its detail page.\n             - **On the company detail page**, scrape the following 19 fields:\n                 - Company Info: association, association_id, company_name, branch_office, address, city, State, country, website, company_phone, company_fax, company_email\n                 - Contact Person Info: contact_name, contact_title, contact_email, contact_phone, contact_whatsapp, contact_wechat, contact_mobile\n             - Collect *all* contact persons listed on the detail page, creating a separate record for each contact (repeating company info).\n             - After scraping, navigate *back* to the search results page.\n        iv. If the URL *is* already in the visited list, skip this company and proceed to the next one.\n8.  **Format Output:** Consolidate all the scraped contact records (one per contact person, up to the 10-company limit) into a single JSON list of objects. Each object must contain all 19 fields specified above. Use an empty string \"\" for any missing values.\n9.  **Return:** Return *only* the final JSON list.\n",
    "task_id": "1356330",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n\n    1) Objective\n    - Extract MITRE tags from the source: https://attack.mitre.org/matrices/enterprise/cloud/\n    - Based on the input json data.\n    2) Required Tags\n    - tactics\n    - techniques\n    - sub-techniques\n    - reasoning (a brief explanation of how the entries were identified)\n    3) Output Format\n    - Provide a JSON object containing the information above\n    4) JSON Output Template\n    {\n    \"tactics\": {\"id\":\"example\",\"name\": \"example\"},\n    \"techniques\": {\"id\":\"example\",\"name\": \"example\"},\n    \"sub-techniques\": {\"id\":\"example\",\"name\": \"example\"},\n    \"reasoning\": \"example\"\n    }\n\n    give me the analysis of the data:\n    {\n        \"id\": \"649bfb15326da22d0efca229\",\n        \"name\": \"Storage - Bucket Ransomware - Upload Encrypted File to Bucket\",\n        \"description\": \"This module interacts with the Google Cloud Storage service to perform various operations, including downloading files from a bucket, encrypting files using XOR encryption, uploading encrypted files to a bucket, and removing files from a bucket. It requires the necessary credentials to establish a connection and perform operations. Potential abuse of this functionality by an attacker could involve unauthorized access to files in the bucket, unauthorized encryption of files with malicious intent, unauthorized uploading of encrypted or malicious files to the bucket, or unauthorized removal of files from the bucket. This can lead to the storage of malicious or unauthorized content, data loss, compromise of sensitive information, or disruption of critical operations.\",\n        \"group\": \"Assume Breach [GCP]: SIEM Validation - Detection of High-Privilege Activities\"\n    }\n\n",
    "task_id": "202609",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Go to https://celltrader.co.za/shop/ it is a woocommerce store, Extract as much information as possible from each product saving the data each time. Scroll down 1 page at a time until you reach the end of the page. Go to the next page and do the same until there are no pages left",
    "task_id": "191902",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://artificialanalysis.ai and follow the prompt: Analyze this website comprehensively. Extract all information about API comparisons, pricing data, quality metrics, and performance benchmarks for text generation, image generation, and other AI services. Focus on identifying the top-performing APIs in terms of quality vs price ratio.",
    "task_id": "2202304",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Goal: Analyze authentication implementation and the main server entry point\nBackground motivation: Need to understand how authentication is implemented and how the server is initialized\nExpected output format: Detailed analysis of authentication implementation and server initialization code\n\nCandidate URLs: \n- https://github.com/makenotion/notion-mcp-server/blob/main/src/openapi-mcp-server/auth/index.ts\n- https://github.com/makenotion/notion-mcp-server/blob/main/src/init-server.ts\n- https://github.com/makenotion/notion-mcp-server/blob/main/src/openapi-mcp-server/index.ts",
    "task_id": "1327246",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://play.google.com/store/apps/details?id=com.greentech.quran and follow the prompt: Extract competitive information about Al Quran (Tafsir & by Word) app including: rating, reviews, downloads, developer info, key features, pricing model, and what makes it unique compared to other Quran apps. Focus on competitive differentiation.",
    "task_id": "1766206",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "search https://www.redfin.com for homes in 70125 and find homes that have been on the market for 180 days are longer and that are multifamily also produce the Puppeteer script used to do this with a script",
    "task_id": "451705",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n        Find the website for Georgia Superior Court Clerks' Cooperative Authority (GSCCCA) UCC Filings and search for the company '1 OAK Roofing' using the method: Search the GSCCCA UCC index by debtor name ('1 OAK Roofing'). Expected documents: UCC-1 Financing Statements, UCC-3 Amendments/Continuations/Terminations..\n        Extract the following attributes: debtor information (company name, address), secured party information, associated individuals (potential owners/management hints from filing details).\n        Export all found information in structured JSON format.\n        You must include the URL you extracted the data from in the results.\n        Do not try to login to any website. You do not have credentials for any login.",
    "task_id": "1220939",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n        Go to https://www.machinerytrader.com/listings/search?Category=1055&sort=9\n\n        If a captcha is encountered, wait, I will solve it, then continue.\n\n        For each page of search results (at least 5 pages):\n            Extract all skid-steer listings that are in these states: ['Idaho', 'Utah']\n\n            For each matching listing, immediately output the following information in JSON format:\n            {\n                \"category\": \"The category (e.g. Track Skid Steers, Wheel Skid Steers)\",\n                \"equipment_name\": \"The equipment name/model\",\n                \"hours\": \"Number of hours on the machine\",\n                \"current_bid\": \"Current bid if available\",\n                \"price\": \"Price if available\",\n                \"num_bids\": \"Number of bids on the item\",\n                \"time_remaining\": \"Time remaining in auction\",\n                \"location\": \"Location (city and state)\",\n                \"seller_name\": \"Seller name\",\n                \"seller_phone\": \"Seller phone number\",\n                \"auction_url\": \"The URL of this specific auction\"\n            }\n\n            Output each listing as soon as you find it, don't wait to collect all listings.\n            Do not include any \"Mini Skid Steers\" in the results.\n\n            After outputting a listing, continue to the next one.\n\n            After processing all listings on a page, go to the next page and repeat.\n        ",
    "task_id": "944335",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Develop a robust web scraping script to systematically extract detailed business information from the Better Business Bureau (BBB) website for companies categorized under \"Access Control Systems\" across all available pages. The script should efficiently handle pagination, collect unique business profile URLs, and extract specified data fields from each profile. The final dataset should be stored in a structured JSON file.\n\nDetailed Requirements:\n\nPagination Handling:\n\nInitiate the scraping process at the following URL:\nperl\nCopy\nEdit\nhttps://www.bbb.org/search?find_country=USA&find_text=access%20control%20systems&find_type=Category&page=1&sort=Relevance\nImplement logic to navigate through all search result pages by incrementing the page parameter until no further results are available.\nBusiness Profile URL Collection:\n\nOn each search results page, identify and collect the URLs of individual business profiles.\nStore these URLs in a list, ensuring that each URL is unique by removing duplicates.\nData Extraction from Business Profiles:\n\nFor each unique business profile URL, access the page and extract the following information:\nBusiness Name: The official name of the business.\nWebsite URL: The business's website address.\nPhone Number: Contact phone number.\nCategory: Business category (e.g., Access Control Systems).\nAddress: Physical address of the business.\nData Storage:\n\nOrganize the extracted data into a JSON file, with each business entry structured as follows:\njson\nCopy\nEdit\n{\n  \"business_name\": \"Example Business Name\",\n  \"website_url\": \"http://www.example.com\",\n  \"phone_number\": \"(123) 456-7890\",\n  \"category\": \"Access Control Systems\",\n  \"address\": \"1234 Example St, City, State, ZIP\"\n}\nError Handling and Logging:\n\nImplement robust error handling to manage scenarios such as missing data fields, HTTP errors, or timeouts.\nMaintain a log file to record the status of each processed URL, including any errors encountered, to facilitate debugging and ensure data integrity.\nCompliance and Ethical Considerations:\n\nEnsure that the scraping activities comply with the BBB's terms of service and legal guidelines.\nImplement respectful crawling practices by including appropriate delays between requests to avoid overwhelming the website's servers and to mimic human browsing behavior.\nAdditional Considerations:\n\nEfficiency: Optimize the script to minimize execution time while ensuring thorough data collection.\nScalability: Design the script to handle a large number of listings and pages without performance degradation.\nMaintainability: Write clean, well-documented code to facilitate future updates or modifications.\nBy adhering to these detailed requirements, the script will effectively gather and store comprehensive information on businesses offering access control systems, facilitating further analysis or utilization of the data as needed.",
    "task_id": "503786",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Goal: Find technical details of the vulnerability from GitHub or the original pull request that fixed it\nBackground motivation: I need to find the specific technical details of the vulnerability from the source, including the exact fix that was applied and any discussions around it.\nExpected output format: Technical details of the vulnerability, including code snippets if available, the exact fix implemented, and any discussions from the maintainers about the vulnerability.\n\nCandidate URLs: \n- https://github.com/ljharb/qs/pull/428\n- https://github.com/n8tz/CVE-2022-24999",
    "task_id": "1704522",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Search for the latest suggested phones on amazon.com and extract details including names, specifications, and prices.\n\nExpected Output: A list of latest suggested phones with names, specifications, and prices.",
    "task_id": "448768",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Go to \"https://www.tenderned.nl/aankondigingen/overzicht\", search for \"Digitale Transformatie\". For each posting in the list open the page and extract title, description and the value of the contract. The page is in Dutch.\n",
    "task_id": "1513697",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "You are a helpful assistant that visits web pages and extracts relevant information about enzyme transformations of n-Hexane, including enzyme names, EC numbers, genes/proteins, transformation reactions, and resulting metabolites. Visit the NCBI Bookshelf page about n-Hexane toxicity and extract detailed information about its metabolism, focusing on enzyme transformations, enzyme names, EC numbers, genes/proteins involved, and metabolic pathways.",
    "task_id": "1380258",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "site:pertanian.go.id ISPO certified companies list oil palm plantation coordinates polygons",
    "task_id": "1019626",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "email address or contact form for restaurants listed on restaurantsofmanchester.com",
    "task_id": "808075",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n            Task: Scrape property listings from a pre-filtered Yad2 search results page\n\n            Step 1: Go directly to this URL: https://www.yad2.co.il/realestate/rent?multiNeighborhood=855%2C159%2C163&rooms=2-3&price=4000-8000&zoom=14\n            \n            Step 2: Wait for the page to fully load with all property listings\n            \n            Step 3: Extract detailed data from each property listing:\n            - Property ID (from listing URL or data attribute)\n            - Title (property type and rooms)\n            - Price\n            - Address (City, Neighborhood, Street)\n            - Property details (Type, Rooms, Floor, Area in sq.m)\n            - Image URLs\n            - Description (full text)\n            - Date listed\n            - Contact information (name, phone if available)\n            \n            Step 4: Check if there are more pages and note whether there's a next page\n            \n            IMPORTANT: Format each listing with \"LISTING START\" and \"LISTING END\" markers so I can parse them.\n            Include the full details in a structured format like:\n            LISTING START\n            ID: [id]\n            Title: [title]\n            Price: [price]\n            City: [city]\n            Neighborhood: [neighborhood]\n            Street: [street]\n            Type: [property type]\n            Rooms: [rooms]\n            Floor: [floor]\n            Area: [area]\n            Description: [description]\n            Date Listed: [date]\n            Contact: [name]\n            Phone: [phone]\n            Images: [urls]\n            LISTING END\n            ",
    "task_id": "1063981",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.susannabeatrice.global and follow the prompt: Extract all website content including text, services offered, about/biography information, contact details, portfolio/work examples, testimonials, and any other relevant content that can be used to populate a website. Also analyze any branding elements, color schemes, and content structure.",
    "task_id": "2252317",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "go to amazon.com and search for puma shoes.\nafter scape the data of first 15 result of shoes. Get their images, title, description and price of each shoes.\nOutput data must be in tabular format.",
    "task_id": "372770",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Extract the following information from the provided product page:\nURL: https://www.adidas.jp/%E3%82%B3%E3%83%BC%E3%83%89%E3%82%AB%E3%82%AA%E3%82%B9-%E3%83%9C%E3%82%A2-25-codechaos-boa-25/IH5142.html?pr=product2_rr&slot=2&rec=ds\n\n- `optionGroups`: A list of option groups available for the product. Each option group contains:\n  - `name`: The name of the option group (e.g., Colours, Sizes).\n  - `options`: A list of options in the group. Each option contains:\n    - `name`: The name of the option (e.g., Black, Medium).\n    - `imageUrl`: The URL of the image associated with the option, if available.",
    "task_id": "225377",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n            Find the website for Pennsylvania Department of State - Corporation Bureau or use Google to search for the Core Roofing on Pennsylvania Department of State - Corporation Bureau (this method works best with non goverment websites such as linkedin).\n            Search for the company 'Core Roofing' with context: {\"location\": \"289 W Uwchlan Jave, Downingtown PA 19335\"} using the method: Search by company name 'Core Roofing' and/or address '289 W Uwchlan Jave, Downingtown PA 19335'. If available, search by business ID or registration number. Check for variations of the name (e.g., Core Roofing LLC, Core Roofing Inc.)..\n            Extract the following attributes: registered agent, officers, directors, entity status, business type, date of incorporation/formation.\n            Export all found information in structured JSON format.\n            You must include the URL you extracted the data from in the results.\n            Do not try to login to any website. You do not have credentials for any login.  \n            If at any point you get blocked by cloudflare, attempt to bypass it.  If you are unable to do so STOP.",
    "task_id": "1291834",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\nYou are a compliance analyst specialized in e-commerce due diligence.\nYou havr to gather key information to enable a compliance analyst to assess if a website is a dropshipping website.\nBrowse the folowing url:argotstudio.com\nReturn the following information. You need to actuelly return the information, not a link to the information.\n- location: is there a physical address of the company or the warehouse ? Could you determine from where the website is operated ?\n- return policy: read the return policy and describe it. In particular, are returns allowed ? Are they charged ?\n- delivery time: read the delivery policy or the FAQ, or the terms and conditions. what are the typical delivery times ? If delivery times are long (>1 week), is it justified ?\n- product photo: are the product photos generic ? Could the product be sold on aliexpress ? You can go on aliexpress if needed.\n",
    "task_id": "1501614",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Goal: Find information about China's National Intelligence Law and requirements for companies to assist with state intelligence work\nBackground motivation: Need to understand specific legal requirements for Chinese companies to cooperate with government authorities and share data when requested\nExpected output format: Specific articles and provisions from China's National Intelligence Law requiring companies to cooperate with authorities, along with analysis of implications\n\nCandidate URLs: \n- https://www.lawfareblog.com/beijings-new-national-intelligence-law-defense-offense\n- https://digitalcommons.law.scu.edu/cgi/viewcontent.cgi?article=3169&context=historical\n- https://thediplomat.com/2019/02/the-real-danger-of-chinas-national-intelligence-law/",
    "task_id": "1748573",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "1. Go to Google Maps Preview\n2. Search for \"BP Rotterdam Europoort\" click on the first recommendation below the searchbox and wait for the map to update\n3. Click on the \"Hotels\" button\n4. Search for \"vakantiepark\" in the search field\n5. For each result in the list:\n   - Click on Ergebnisse list\n   - Extract all available information including:\n     * Name\n     * Address\n     * Rating\n     * Number of reviews\n     * Email (if available)\n     * Phone number\n     * Website\n     * Description\n6. Save all collected data in a structured JSON format using the Accommodation model",
    "task_id": "227142",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\nTask: Get 10 vendors with name, email, address, contact_no, rating, and vendor_distance.\nRetrieve complete details for the 10 bicycle tire repair vendors near Uppsala, 75332, prioritizing email extraction. At least 10 vendors must include validated emails.\n\nSearch & Initial Data Collection:\n1. Search Google Maps for \"Bicycle tire repair service vendors in Uppsala, 75332\".\n2. Collect the top 10 closest vendors with:\n   - vendor_name\n   - address\n   - contact_no\n   - rating (e.g., \"4.5\")\n   - vendor_distance (e.g., \"1.2 km\")\n   - listed_website (if available on Google Maps).\n\nStep 1: For vendors with a listed website:\n   - Click on the vendor's listing to access their detailed information.\n   - Navigate to the listed website.\n   - Collect the website links for each vendor.\n   - If no website is listed, proceed with the next vendor.\n\nOutput Requirements:\n{\n  \"vendors\": [\n    {\n      \"vendor_name\": \"Example Vendor\",\n      \"website\": \"http://www.examplevendor.com\",\n      \"contact_no\": \"+46 18-123 4567\",\n      \"address\": \"Street 1, 75332 Uppsala\",\n      \"rating\": \"4.6\",\n      \"vendor_distance\": \"0.5 km\"\n    },\n    ...\n  ]\n}\n",
    "task_id": "325376",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Extract job listings from the Upwork job search page. For each job, go to the job page, extract the title, description, budget, and number of applicants. Then move on to the next job and repeat the process.",
    "task_id": "1100384",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://tris.vn/ and follow the prompt: Analyze this website comprehensively. Extract:\n1. Overall layout and structure\n2. Content sections and their hierarchy\n3. Current design elements (colors, fonts, styling)\n4. Navigation structure\n5. Key features and functionality\n6. Images and media used\n7. Any interactive elements\n\nProvide a detailed analysis of the website's current state so I can recreate and improve it.",
    "task_id": "2118996",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Visit the URL https://verificacfdi.facturaelectronica.sat.gob.mx/default.aspx?id=B4BB65A2-3346-4988-BB10-124524B20148&re=PMO131216PM8&rr=XAXX010101000&tt=369900.000000&fe=HIl4RQ== solve the captcha, and return the table as JSON",
    "task_id": "453534",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "On the current Amazon Best Sellers in Books page, identify all book listings. Each book listing is typically marked with a rank (e.g., #1, #2). For each book listing, extract the following information:\n1.  **Title**: The main, larger text link for the book.\n2.  **Author**: The smaller text directly below the title.\n3.  **Review_Count**: The number located next to the star rating.\n4.  **Price**: The value that starts with a dollar sign ($).\nPresent the extracted information as a list of JSON objects. Each object should represent a book and contain the keys 'Title', 'Author', 'Review_Count', and 'Price'. Ensure all books visible on the current page are extracted.",
    "task_id": "2143378",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n    Go to https://szigetfestival.com/en/tickets and scrape the webpage to extract all available ticket pricing information. For each ticket type, collect the ticket name, price, ticket category (e.g., day pass, multi-day pass, camping upgrade), availability status, and any relevant descriptions or benefits associated with the ticket. If the page uses dynamic content loading, ensure all ticket options are fully visible before scraping. This may involve scrolling or waiting for JavaScript-rendered elements to finish loading.\n\n    If there is a cookie consent banner or other interactive element that blocks the page content, dismiss it to allow full access to the ticket listings. Make sure all pricing details are accurately captured, including multiple pricing tiers or early bird offers where applicable. Avoid duplicates and confirm that all necessary information is parsed correctly.\n\n    Return the extracted data in a structured format such as JSON or CSV, with clear and descriptive field names like ticket_name, category, price, availability, and description to ensure readability and usability.",
    "task_id": "1122929",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\ngo to this url and give me the Complete balance sheet of this company : https://www.tcs.com/content/dam/tcs/investor-relations/financial-statements/2023-24/ar/annual-report-2023-2024.pdf\n",
    "task_id": "976402",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n\n### Step 1: Navigate to the website\n- Open [RBI Bulletin] (https://www.rbi.org.in/Scripts/BS_ViewBulletin.aspx)\n\n### Step 2: Open Bulletins for year - 2025 and month - 1\n- Click on 2025\n- click on 1 under the list\n\n### Step 3: Open Excel File attached to heading 'Select Economic Indicators' under heading 'Current Statistic General'\n- Open excel attachment associated with heading 'Select Economic Indicators' under heading 'Current Statistic General'\n- Extract latest Policy Repo Rate\n\n\n",
    "task_id": "1612606",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://remote.team/en/ and follow the prompt: Analyze this website comprehensively. I need to understand:\n1. Overall structure and navigation\n2. All main pages and sections\n3. Content types and layouts\n4. Interactive features and functionality\n5. Design elements, colors, typography\n6. Any forms or dynamic content\n7. Footer and header elements\n\nPlease explore multiple pages to get a complete understanding of the site architecture.",
    "task_id": "1960499",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Revised Prompt:\n\nObjective:\n\nAutomate the process of searching the California Attorney General's Proposition 65 60-Day Notice Search website for active or past legal cases filed against a specific company, focusing on whether those cases involve specific products. Extract relevant case information first, then match against the provided product list, and finally, output the results in a structured JSON format.\n\nInputs:\n\ncompany_name: The name of the company to search for (string).\nproducts: A list of exact product names to check for in the case records (list of strings).\nWorkflow:\n\nPrepare Company Name:\n\nTake the input company_name.\nRemove any of the following corporate suffixes from the end of the name (case-insensitive): Inc, Incorporated, Corp, Corporation, LLC, Ltd, Limited, PLC.\nTrim any leading/trailing whitespace from the resulting name.\nUse this cleaned name for the search.\nExample: \"Mezzaten Inc\" becomes \"Mezzaten\"; \"GreenWorld LLC\" becomes \"GreenWorld\".\nNavigate and Search:\n\nNavigate and Search:\nGo to the specified URL: https://oag.ca.gov/prop65/60-day-notice-search\nLocate the \"Search Notices\" section of the page, specifically the form containing the \"Defendant Name\" input field. Ensure this section is interactable (simulating scrolling if necessary to bring it fully into view).\nInput the cleaned company name into the \"Defendant Name\" field within this specific form.\nCrucially: Do not input any values into any other search fields (like Date Received, Source, Noticing Party, etc.) within this form. Leave them blank.\nInitiate the search. This is very important: There may be multiple 'Search' buttons on the page. You must click the specific 'Search' button that is directly associated with the 'Search Notices' form (the form containing the 'Defendant Name' field you just filled) and is typically located at the bottom of that specific form section. Do not click any other search buttons on the page.\nWait for the search results table, associated with the \"Search Notices\" form, to load completely.\n\nInitialize an empty list to store details from all found cases (e.g., extracted_cases).\nSystematically process the search results:\nIf results span multiple pages, navigate through every page.\nFor each row (representing a single case notice) in the results table on all pages:\nExtract the value from the \"Date Received\" column. Store this as date_filed.\nExtract the exact text from the \"Source\" column. Store this as source_text.\nAdd an object or record containing { \"date_filed\": date_filed, \"source_text\": source_text } to the extracted_cases list.\nContinue this extraction until all cases across all pages for the given company search have been processed.\nMatch Products and Prepare Output Data:\n\nInitialize an empty list for the final output structure (e.g., output_product_list).\nIterate through each product_name provided in the input products list.\nFor the current product_name:\nSearch within the extracted_cases list (collected in Step 3) for any entry where the source_text exactly matches the current product_name.\nIf a match is found:\nIdentify the corresponding date_filed from the first matching entry found in extracted_cases.\nCreate a result object: { \"product_name\": product_name, \"date_filed\": matched_date_filed, \"case_filed\": true }\nAdd this object to the output_product_list.\nIf no match is found after checking all entries in extracted_cases:\nCreate a result object: { \"product_name\": product_name, \"case_filed\": false }\nAdd this object to the output_product_list.\nGenerate Final Output:\n\nConstruct the final JSON object using the output_product_list prepared in Step 4. The structure must be:\nJSON\n\n{\n  \"products\": [\n    // ... entries from output_product_list go here\n  ]\n}\nFinal Execution Rules:\n\nEnsure all interactions target the correct elements on the webpage, especially the \"Defendant Name\" field within the \"Search Notices\" section.\nHandle potential loading delays gracefully when waiting for search results and navigating pages.\nOnce the final JSON output is constructed based on the complete analysis of extracted cases against the input product list, terminate the process immediately.\nDo not perform any further actions, searches, or retries after generating the output.",
    "task_id": "1662847",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Extract absolutlyall the information related mobiles offers of a telecom operator.\n    extract informations only from URLs (take the value off this URLS) :\"{\\\"names\\\": [\\\"Forfait Free 5G\\\", \\\"S\\u00e9rie Free\\\", \\\"Forfait 2\\u20ac\\\", \\\"Free Flex\\\"], \\\"URLs\\\": [\\\"//mobile.free.fr/fiche-forfait-free\\\", \\\"//mobile.free.fr/fiche-forfait-serie-free\\\", \\\"//mobile.free.fr/fiche-forfait-2-euros\\\", \\\"//mobile.free.fr/free-flex\\\"]}\". \n    Your role is to extract all the details from these URLs by mobile offer name. \n    You need to go through each URL to complete all the information about these mobile offers.\n    Your final answer must be in a json format",
    "task_id": "769424",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Navigate to https://www.eib.org/en/projects/pipelines/all/20230590 and extract links to tender-specific documents such as project specifications, technical drawings, or contractual agreements. Ignore irrelevant documents such as terms and conditions, privacy policies, and marketing brochures. Also ignore domain-brad documents.",
    "task_id": "1202901",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "go to https://www.comparefirst.sg, click term insurance, click non-DPI, set date of birth as 01/01/1990, gender male, smoker no, select policy term 21 to 30,  sum assured 1,000,000, critical illness NO, then search and extract all the insurancce companies quotes and summarise into a csv file on the company name, premium rate, policy term",
    "task_id": "426639",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "1. Go to url https://www.nseindia.com/companies-listing/corporate-filings-financial-results-comparision\n\t\t\t2. type Tata Consultancy Services Limited in company name\n\t\t\t3. seclect name from dropdown\n\t\t\t4. press search\n\t\t\t5. extract data in the table,\n\t\t\t6. scroll till you capture whole table\n\t\t\t7. parse in json format.\n\t\t\t",
    "task_id": "265470",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n    **Job Search Instructions**\n    1. Google Search:\n       - Search: 'site:linkedin.com OR site:glassdoor.com AI Engineer jobs in Pakistan'\n       - If any pop-ups appear (cookies, newsletters, sign-in prompts):\n         * Immediately close using X button\n         * Never accept or subscribe\n         * If multiple pop-ups, close sequentially from top-right\n    \n    2. Link Navigation:\n       - First collect 3 LinkedIn job links from search results\n       - Then collect 3 Glassdoor job links\n       - Prioritize links with:\n         * Job title matching search terms\n         * Recent post dates (<7 days)\n         * Direct apply options\n    \n    3. Job Page Interaction:\n       For each visited job page:\n       a. Close any overlay pop-ups within 2 seconds\n       b. Scroll through entire job description\n       c. Extract these details:\n          - job_title (text)\n          - company (employer name)\n          - experience (required years)\n          - jobNature (Full-time/Part-time)\n          - location (specific office address)\n          - salary (range or exact figure)\n          - apply_link (direct URL)\n        d: if these details are not found click the job and try to extract information\n    \n    4. Data Structure:\n       Return exactly 6 jobs (3 LinkedIn + 3 Glassdoor) as:\n       [\n         {\n           \"job_title\": \"Python Developer\",\n           \"company\": \"Tech Corp\",\n           \"experience\": \"3-5 years\",\n           \"jobNature\": \"Full-time\",\n           \"location\": \"New York, NY\",\n           \"salary\": \"$80,000 - $120,000\",\n           \"apply_link\": \"https://linkedin.com/job/123\"\n         },\n         ...\n       ]\n       Ensure valid JSON format with double quotes else all the data will be invain incase of failure go throuth the response and extract information from response of the searchs activity\n    ",
    "task_id": "930386",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n            Find the website for Tennessee Department of Commerce and Insurance or use Google to search for the Armor Roofing of Tennessee on Tennessee Department of Commerce and Insurance (this method works best with non goverment websites such as linkedin).\n            Search for the company 'Armor Roofing of Tennessee' with context: {\"location\": \"545 N Mt Juliet Rd Suite 1201 TN Mount Juliet 37122 US\"} using the method: Search for 'Armor Roofing of Tennessee' in the department's licensing database to verify roofing contractor licenses and any associated information..\n            Extract the following attributes: licenses, certifications, disciplinary actions.\n            Export all found information in structured JSON format.\n            You must include the URL you extracted the data from in the results.\n            Do not try to login to any website. You do not have credentials for any login.  \n            If at any point you get blocked by cloudflare, attempt to bypass it.  If you are unable to do so STOP.",
    "task_id": "1254332",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "go to www.autotrader.co.uk\naccept all cookies\nuse the \"find your perfect car\" form\nenter a postcode of BT817YJ\nselect volkswagen from the list of makes\npause for a few seconds to allow the model list to update\nselect id.4 from the list of models\nfrom the list of max price select the price nearest to 20000\nsearch the available cars and list them including the price, the cost, their location and a link to the URL for the ad for the car\n",
    "task_id": "1024420",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "'You are a professional job finder.\n\t\t\t\tGo to https://upwork.com then: \n\n\t\t\t\t'find Python Django jobs on Upwork.com and save them to a file'\n\t\t\t\tImportant:\n\t\t\t\t- Wait for each element to load before interacting\n\t\t\t\t- Make sure the application is typed exactly as shown\n\t\t\t\t- Verify the post button is clickable before clicking\n\t\t\t\tllm=ChatOpenAI(model=\"gpt-4o\"),\n\t\t\t\tbrowser_context=browser_context,\n\t\t\t\t",
    "task_id": "919843",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.allabolag.se/segmentering and follow the prompt: Try a different approach to find HVAC companies with high revenue:\n1. First, apply the HVAC industry filter (Ventilation, luftbehandling)\n2. Try setting the revenue filter with different values or methods (500000000, 500,000,000, or 500 miljoner SEK)\n3. If revenue filter doesn't work properly, get the list of all HVAC companies and then sort by revenue\n4. Look for the largest companies in the HVAC sector and identify those with revenue above 500M SEK\n5. Extract company names, revenue figures, and basic information",
    "task_id": "2185648",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Go to https://www.autotrader.ca/dealersearch/ and extract dealer information from multiple pages.\n\n1. For each dealer card on the current page, extract:\n   - Dealer name (e.g., \"613 Motorz\", \"613 Rides\")\n   - Website URL - Look for and click \"View Dealer Website\" button if available (THIS IS THE MOST IMPORTANT)\n   - Complete address including postal code\n   - Phone number\n\n2. After extracting all dealers from the current page:\n   - Instead of clicking pagination controls, construct a direct URL for the next page\n   - Use this URL pattern: https://www.autotrader.ca/dealersearch/?pg=N where N is the page number\n   - For example, page 2 would be: https://www.autotrader.ca/dealersearch/?pg=2\n   - Navigate to this URL using go_to_url action\n   - Wait for the new page to load completely\n\n3. Repeat steps 1-2 for at least 5 pages (pages 1-5), using the direct URL approach for navigation.\n\nIMPORTANT: Do NOT try to click pagination controls - the site uses Angular and the pagination may not be easily clickable. Instead, directly modify the URL with the pg parameter to navigate to different pages.",
    "task_id": "1099868",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Go to https://webgate.ec.europa.eu/rasff-window/screen/search search for Morocco (origin), collect the notifications in the last year and generate a csv file of the output (even if you didn't collect all the notifications)",
    "task_id": "823668",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Identify the latest cybersecurity best practices for cloud computing transformation on the NIST website by navigating directly to the Cloud Computing section and summarizing the key recommendations",
    "task_id": "795554",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.sec.gov/Archives/edgar/data/0001876042/000119312525132755/d737521ds1a.htm and follow the prompt: Extract the financial statements from this S-1/A filing. Look for tables with titles like 'Consolidated Balance Sheets', 'Consolidated Statements of Operations', and 'Consolidated Statements of Cash Flows'.",
    "task_id": "2125188",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.idx.co.id/StaticData/NewsAndAnnouncement/ANNOUNCEMENTSTOCK/From_EREP/202404/e8896172d8_1ec8334461.pdf and follow the prompt: Go to the Kalbe Farma Annual Report 2023. Find the section on the Consumer Health Division. Extract the key financial data for this division, including sales, growth, and profitability. Also, look for any information on new product launches or marketing strategies in the beverage segment.",
    "task_id": "2179094",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n    You are a helpful assistant that validates if an issue detected by an upstream system is a real issue on the website or not.\n    You will be given an issue and a website url.\n    You will need to check if the issue is a real issue on the website or not.\n    You will need to use both the text content and visual screenshots of the page to validate the issue.\n    If two products are compared, or two page previews (example: product previews in a category page) are compared and show different details, click on both to go in depth into comparing, just like a human would.\n    Basically you're doing an in-depth analysis of the error, and you're trying to find out if the issue is a real issue on the website or not.\n    You will need to return a detailed markdown report of your evaluation, and if you couldn't evaluate if the issue is a real issue or not, you should also return a reason why you couldn't evaluate it.\n    Every time, whatever happens, you should return a report at the end.\n\n    Here is the issue:\n    {\n  \"title\": \"Resolve Contradictory Read/Write Speeds Between Product Specification and User Review\",\n  \"severity\": \"low\",\n  \"status\": \"open\",\n  \"page_type\": null,\n  \"regions\": [\n    \"US\"\n  ],\n  \"platforms\": [\n    \"desktop\",\n    \"mobile\"\n  ],\n  \"description\": \"Official specs list 6700 MB/s read and 5300 MB/s write, but a highlighted user review states 6000 MB/s write and 5300 MB/s read, reversing the numbers and creating a clear factual contradiction that may mislead shoppers.\",\n  \"shortlink_url\": \"https://www.bhphotovideo.com/c/product/1871607-REG/lacie_stna4000400_4tb_rugged_ssd_pro.html\",\n  \"shortlink_title\": \"4TB Rugged SSD Pro\",\n  \"issue_rows\": [\n    {\n      \"row_type\": \"image\",\n      \"image_url\": \"https://focal-dashboard.s3.amazonaws.com/issue_creation/76ee887d-4ab2-4d4e-ae35-2dc99fab0fc1.webp\"\n    },\n    {\n      \"row_type\": \"image\",\n      \"image_url\": \"https://focal-dashboard.s3.amazonaws.com/issue_creation/f9be2621-5a73-4260-86bd-d7b8ac626056.webp\"\n    }\n  ]\n}\n\n    Here is the url to check on:\n    https://www.bhphotovideo.com/c/product/1871607-REG/lacie_stna4000400_4tb_rugged_ssd_pro.html\n    ",
    "task_id": "2357258",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "go to zillow and draft all rental listings listed by property owner in brentwood, ca put the results in a blank text document on a free notepad website",
    "task_id": "21962",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.icis.com/explore/commodities/chemicals/ and follow the prompt: Navigate to the website and find information about their data and API offerings. I'm looking for details on what data is available, how it can be accessed (API, data feeds, etc.), and any developer documentation. Extract this information and save it to a file.",
    "task_id": "2307496",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://SlimSista.com and follow the prompt: Analyze the brand and style guidelines of SlimSista.com including:\n1. Color scheme and branding colors\n2. Typography and font choices\n3. Overall design aesthetic and style\n4. Layout patterns and design elements\n5. Logo and brand imagery\n6. Content structure and organization\n7. User experience patterns\n8. Any specific design elements that define the brand identity\nTake detailed notes of all visual and stylistic elements for creating an ebook that matches this brand.",
    "task_id": "2236188",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n    0. Go to https://www.daraz.pk/, handle any captcha/security if shown.\n    1. Search for 'wireless headphones'.\n    2. Browse and extract product information from the first two pages.\n    3. For each product, extract:\n        - Product Name\n        - Price\n        - Number of Sold units\n    4. Identify top best-selling products.\n    5. Provide a summary table with:\n        - Product Name\n        - Price\n        - Number Sold\n    ",
    "task_id": "1485252",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "go to amazon.com, search laptops, go to each laptop page and grab info of (Name,RAM,GPU,Processor,Display,Weight,Storage,Screen_Type,Refresh_Rate,Operating_System,Color,Price,Rating,URL), do it for 2 amazon pages",
    "task_id": "122404",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Analyze MCC token smart contract code on BSC to identify functions, ownership, and potential risks with urls or queries: ['https://bscscan.com/token/0x700735317e1af4687c17f5c30e11a74778395922#code']\n\nFor Context: Navigate to BscScan contract page for MCC token. Analyze the contract code to:\n1. Check if contract is verified\n2. Identify all READ functions and their purposes\n3. Identify all WRITE functions and analyze risks\n4. Check for mint, burn, pause, blacklist functions\n5. Analyze owner privileges and access control\n6. Look for any backdoors or suspicious code\n7. Check for proxy pattern or upgradability\nCreate an artifact file named 'mcc_bsc_contract_code_analysis.md' with detailed function analysis.",
    "task_id": "1797096",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.ptechpartners.com/ and follow the prompt: Navigate to this website and find the About Us, Team, Leadership, or Staff pages. Extract all named contacts with their job titles and email addresses if available. Look for team member directories, executive profiles, and any contact information.",
    "task_id": "1873462",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "go to B0DQY9DY79 and look for the product price and give it to me in the results along with product name and seller.",
    "task_id": "229756",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Goal: Get specific details about freetype module's CMake configuration and how to include it in a container build\nBackground motivation: Need to provide exact information about the dependency status of freetype in OpenCV 4.10.0 and how to include it in a container build.\nExpected output format: The exact CMake configuration showing how freetype dependency is handled, and specific instructions for including it in a container build.\n\nCandidate URLs: \n- https://github.com/opencv/opencv_contrib/blob/4.10.0/modules/freetype/CMakeLists.txt\n- https://github.com/opencv/opencv/wiki/Docker\n- https://github.com/opencv/opencv_contrib/blob/4.10.0/modules/freetype/README.md",
    "task_id": "1624674",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Go to redfin.com, handle any security checks, search for properties in San Diego, CA. Apply the following filters: Max Price set to $500,000, Bedrooms set to 3+. Since there isn't a direct 'nice view' filter, scan the listing descriptions or details of the filtered results for keywords like 'view', 'ocean', 'bay', 'panorama', 'scenic', 'hilltop'. Collect the URLs of the first 5 properties that match all these criteria (price, bedrooms, and view keywords).",
    "task_id": "1360102",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Scrape the first 20 tenders for UN-related IT projects from the UN Global Marketplace at https://www.ungm.org/Public/Notice",
    "task_id": "1133145",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Goal: Find detailed information about CVE-2024-34459\nExpected output: Detailed information about CVE-2024-34459 from authoritative sources, including technical details and mitigations\n\nCandidate URLs: \n- https://nvd.nist.gov/vuln/detail/CVE-2024-34459\n- https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2024-34459",
    "task_id": "1289271",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.instagram.com/teo__idt/ and follow the prompt: Analyze this Instagram account thoroughly. I need to understand:\n1. What type of content does this account post? (photos, videos, stories, reels, etc.)\n2. What are the main themes/topics of the content? (lifestyle, business, fitness, art, etc.)\n3. What is the posting frequency pattern?\n4. Can you see any engagement metrics (likes, comments) on recent posts?\n5. What appears to be the target audience based on content style?\n6. Are there any captions or hashtags that indicate the content category?\n\nPlease provide a detailed analysis of the account's content strategy and style.",
    "task_id": "2147069",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Scrape the tenders from https://www.act.nato.int/opportunities/contracting/. For each tender, extract the title, reference number, submission deadline, estimated budget, description, eligibility criteria, procedure type, and dedicated URL. Return the results as a JSON array of objects.",
    "task_id": "1145398",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n            Find the website for All Star Roofing LinkedIn Page or use Google to search for the All Star Roofing on All Star Roofing LinkedIn Page (this method works best with non goverment websites such as linkedin).\n            Search for the company 'All Star Roofing' with context: {\"location\": \"550 Carter Ln, Smyrna TN 37167\"} using the method: Search for 'All Star Roofing' on LinkedIn. Review the company's profile page. Check employee profiles for titles and roles..\n            Extract the following attributes: owners, management, number of employees, serviceable area, company description, employee profiles, industry, company size.\n            Export all found information in structured JSON format.\n            You must include the URL you extracted the data from in the results.\n            Do not try to login to any website. You do not have credentials for any login.  \n            If at any point you get blocked by cloudflare, attempt to bypass it.  If you are unable to do so STOP.",
    "task_id": "1244960",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "go to https://marketingsummit.siliconslopes.com/agenda/list/?tribe-bar-date=2025-05-05 and open each \n        talk and extract name of the talk, description of the talk, presenter's information including name, title and company.\n        once you have the information of the presenter, open LinkedIn and search for the presenter by name & company. If found, get the linkedin\n        profile\n        ",
    "task_id": "1406205",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "You are a legal research assistant focused on California Proposition 65 cases. \n\nYour task is to search for active legal cases related to a company on the California Prop 65 case tracking website.\n\n1. Go to the California Prop 65 case search page at https://oag.ca.gov/prop65/60-day-notice-search\n2. Search for the company using its official name: Puritan in the Defendant Name field\n3. Click on the search button (which is at the bottom of the page there will be two serach button one on top and one on bottom scroll to the bottom)\n3. For each case you find, extract the following information:\n   - Case number\n   - Case title\n   - Status (e.g., active, closed, pending)\n   - Description (brief summary of what the case is about)\n   - Whether it relates to any of the products listed below\n\n4. Special attention for these products:\nPuritan's Pride Ginger Root 500 mg\n\nEvaluate whether any of the cases specifically mention these products or product categories that could include them.\n\nIMPORTANT: If the exact company name doesn't return results, you may try a portion of the name, but focus on using the provided official company name as accurately as possible.\n\nReturn your findings in the required format, even if no cases are found.\n",
    "task_id": "1563973",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Go to https://www.google.com/maps/place/UCI+Cinemas+Showville+Bari/@41.0907971,16.8850177,17z/data=!4m8!3m7!1s0x1347e9b7eab37763:0x407625fc0e2e259a!8m2!3d41.0907931!4d16.8875926!9m1!1b1!16s%2Fg%2F11b6g9l0rz?entry=ttu&g_ep=EgoyMDI1MDMwMy4wIKXMDSoASAFQAw%3D%3D and take all the reviews\n",
    "task_id": "917119",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://videogpt.io/ and follow the prompt: I need to create a 10-15 second 3D Pixar-style animated video for \"SNAKE ATTACK\" game splash screen. Please help me use this video generator to create the video with this script:\n\n\"3D Pixar animation style 15-second video: Opens with dramatic black screen, glowing neon text 'IN A WORLD... FULL OF PIXELS...' appears. Cut to pixel art jungle with rustling leaves and ominous 8-bit atmosphere. Sudden dramatic zoom to snake eye close-up - glowing red eyes with fire effects. Show tiny golden pixelated apple floating in slow motion, spinning like the Holy Grail. Pixelated snake slithers fast, slams into walls with retro neon glitch effects and explosion of colorful pixels. Snake grows longer, eyes glowing, dodging tight corners, flying through tunnels made of fruit. Epic finale: snake launches into sky like rocket, spelling out 'SNAKE ATTACK' in glowing letters with energy pulses. Ends with 'Press Start, Snack Warrior' text. Style: energetic neon retro aesthetic, dramatic cinematic lighting, vibrant colors, epic orchestral music with 8-bit chiptune elements.\"\n\nPlease create this video if the service allows it without authentication.",
    "task_id": "2061640",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Go to https://www.docebo.com/customers/ and apply filters for manufacturing industry. Look through the customer stories to find Fortune 500 manufacturing companies with C-level executive quotes and ROI metrics over 200%. Extract specific quotes, company names, executive titles, and ROI figures.",
    "task_id": "2310254",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Extract all product data (name, price, url, image) from all pages of the search results on https://www.bstn.com/us_en/catalogsearch/result/?q=all&categories=Men. Navigate through the pagination until the last page and collect data from each product listed.",
    "task_id": "1625845",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Extract detailed information about the ZOTAC Gaming GeForce RTX 5080 from Amazon, including specifications, features, and customer reviews.",
    "task_id": "1528795",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://en.youlive.ca/vancouver-building/10526-cambie-garden and follow the prompt: Extract the detailed building information from this page in the exact format the user specified:\n- Building Name\n- Full Address \n- City\n- Neighbourhood  \n- For Sale (number of units)\n- Avg Price (with percentage change if shown)\n- Units / Floors\n- Year Built\n- Material\n- Developer\n- Amenities (full list)\n\nAlso analyze the page structure to understand how to systematically extract this data from similar building pages.",
    "task_id": "2092706",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Go to ycombinator.com and find all enterprises under the Winter 2025 (W25) B2B tag. Collect information such as name, description, founder(s), location, and website URL for each enterprise.",
    "task_id": "1136410",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.amazon.com/s?k=rtx+3060+ti+gaming+pc+intel+i7 and follow the prompt: Extract a list of computer models with RTX 3060 Ti and Intel Core i7. For each model, extract: exact processor model, RAM amount and type, storage configuration (SSD/HDD sizes), power supply specifications, case type/size, pre-installed operating system, and price.",
    "task_id": "1672827",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://dtfprinterusa.com/ and follow the prompt: Navigate to the website and perform a technical analysis. Check for site speed, mobile optimization, crawlability (robots.txt, sitemap.xml), and identify the key money pages (homepage, product pages, category pages).",
    "task_id": "2277898",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n        Goto the link https://apna.co/candidate/jobs. This is a job search portal.  \n\n        You have to select 2 filters.\n        \n        1. Under Filters, for Department, if there is a link saying \"Show 38 more >\",click on it and then select \"Domestic Worker\".\n        \n        2. Under Filters, for \"Date posted\", select \"Last 7 days\".\n        \n        Make sure \"Last 7 days\" and \"Domestic Worker\" filters are now showing under Filters.\n        \n        Wait for the results as per the filters.\n        \n        Click on each search result and read the job details and produce a JSON format with the following fields:\n        \n        Job ID\n        Category\n        Job Portal\n        City\n        Link\n       Company\nContact Person name\nContact person phone number\nJob Title\nMin Salary\nMax Salary\nWork Hours\nDays\nNumber of Openings\nJob Expiry Date\nPosted On\nType of Job\nBenefits\nJob Location\nMin Qualification\nExperience Range\nNo Mandatory Exp\nGender Pref\nRequirements\nInfo  \n\n    Try to guess the value of each field from the page content. For Job ID, use the last part of the URL and for Link, use the full URL.\n     \n    Go back to the search results page and repeat the process for the remaining results.\n    \n    Write all the results in a file called output.json\n        \n\n        ",
    "task_id": "1310325",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Visit https://mailchimp.com/pricing/marketing/compare-plans/?currency=EUR and analyze the pricing information.\n                \n                1. Extract all pricing plans/tiers\n                2. For each tier collect:\n                   - Name\n                   - Monthly price\n                   - Description\n                   - Special messages\n                   - the selected contact number\n                   - Maximum subscribers limit\n                \n                3. Check pricing modifiers:\n                   - Contact selectors\n                   - Sliders\n                   - Dropdowns\n                \n                4. Test different contact numbers:\n                   - 1000 contacts\n                   - 5000 contacts\n                   - 10000 contacts",
    "task_id": "839951",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n    Go to https://www.dextools.io/app/en/ether/pool-explorer.\n    Observe the list of newly listed token pairs.\n    Focus on pairs listed within the last 15 minutes.\n    For each such token pair that meets the criteria (liquidity > 50 ETH and age < 15 minutes):\n        1. Extract the token name (e.g., \"MyToken\", not the pair like \"MyToken/WETH\").\n        2. Extract the token's contract address.\n        3. Extract the liquidity in ETH. Convert this to a float.\n        4. Extract the listing age in minutes. Convert this to an int.\n    Compile this information into a list of Python dictionaries. Each dictionary should have keys:\n    'name' (str), 'contract_address' (str), 'liquidity_eth' (float), 'age_minutes' (int).\n    Return this list of dictionaries as a Python-parsable string.\n    For example: \"[{'name': 'TokenA', 'contract_address': '0x123...', 'liquidity_eth': 60.0, 'age_minutes': 10}, ...]\"\n    If no tokens meet the criteria, return an empty list \"[]\".\n    ",
    "task_id": "1656949",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Look for the most current quantum computing research papers sent to ArXiv over the past two days.",
    "task_id": "1125412",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n        Visit https://breaking-bet.com/en/arbs/prematch\n\n        1. Look for the gear/settings icon in the TOP RIGHT corner of the page (it's a circular icon with a gear symbol).\n           This is the filter button. Click on it.\n\n        2. In the filter panel that opens:\n           - First, find and click the \"Select None\" button to uncheck all bookmakers\n           - Then find and check ONLY these specific bookmakers: 1xbet, bet9ja, merrybet, nairabet, betking, melbet\n           - Click the Apply button to apply the filter\n\n        3. If there's a \"show all\" link at the bottom of the opportunities list, click it to see all opportunities.\n\n        4. For each opportunity row, extract:\n           - Profit percentage (e.g., \"0.02%\", \"0.66%\")\n           - Sport type (e.g., \"Basketball\", \"Handball\")\n           - Event details (teams/participants)\n           - Both bookmaker names (e.g., \"SportyBet\", \"NairaBet\")\n           - Bookmaker links (extract href attributes)\n           - Odds values (e.g., \"1.91\", \"2.1\")\n           - Market type (e.g., \"TO\", \"TU\", \"X2\")\n\n        Structure the data in JSON format with these fields:\n        - profit_margin: the profit percentage as a number\n        - sport_type: the sport (e.g., \"Basketball\")\n        - league: the competition name (e.g., \"Euroleague\")\n        - event: the match description (e.g., \"Maccabi Tel-Aviv - Panathinaikos BC\")\n        - market: the bet market type (e.g., \"TO\", \"TU\")\n        - bookmaker1: first bookmaker name\n        - bookmaker1_link: link to first bookmaker\n        - team1: first team/selection\n        - odds1: odds for first selection\n        - bet1: bet type for first selection (e.g., \"TO(171.5)\")\n        - bookmaker2: second bookmaker name\n        - bookmaker2_link: link to second bookmaker\n        - team2: second team/selection\n        - odds2: odds for second selection\n        - bet2: bet type for second selection (e.g., \"TU(171.5)\")\n        \n        Return the data as a JSON array.\n        ",
    "task_id": "1123218",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Go to https://www.similarweb.com/website/\nAnalyze traffic for the website https://revapets.com\nClick the analyze traffic button and wait for the load of the data\nGive me the below info on the website\n- Year Founded\n- Global Rank\n- Total Visits\n- Bounce Rate\n- Pages per Visit\n- Avg Visit Duration\n- Trending of Total Visits Last 3 Months\n- Top Countries\n- Website Traffic Demographics\n- organic search\n- paid search\n\nThe output return in JSON format ",
    "task_id": "1026443",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "tell me all the details about renile startup from crunchbase and its investors, and how much funding they have raised",
    "task_id": "1225015",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "PROMPT - Generate a script to extract all countries and their most recent year and value for electricity access from this url, such that it adheres to the given output schema. It should be robust and handle edge cases. This data will be updated every month, so I need a script to run every month to get the most recent data.\nURL - https://data.worldbank.org/indicator/EG.ELC.ACCS.ZS?name_desc=false\nSCHEMA - {'Country': 'string', 'Most Recent Year': 'integer', 'Most Recent Value': 'float'}",
    "task_id": "966617",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.vals.ai/benchmarks and follow the prompt: \\u68c0\\u67e5\\u6240\\u6709\\u7684\\u57fa\\u51c6\\u6d4b\\u8bd5\\u9875\\u9762\\uff0c\\u83b7\\u53d6\\u5404\\u4e2a\\u6a21\\u578b\\u5728\\u4e0d\\u540c\\u57fa\\u51c6\\u6d4b\\u8bd5\\u4e0a\\u7684\\u6027\\u80fd\\u6570\\u636e\\uff0c\\u7279\\u522b\\u662f\\u6587\\u4ef6\\u4e2d\\u63d0\\u5230\\u7684\\u6a21\\u578b\\uff08\\u5982GPT-4o\\u3001Claude-3.7-Sonnet\\u3001Gemini-2.5-Pro-Exp\\u3001Deepseek-R1\\u3001QwQ-32B\\u7b49\\uff09\\u5728MMLU\\u3001HumanEval\\u3001GSM8K\\u3001C-Eval\\u7b49\\u6d4b\\u8bd5\\u4e0a\\u7684\\u8868\\u73b0",
    "task_id": "1757675",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.beauhurst.com/ and follow the prompt: Extract detailed information about Beauhurst database capabilities including what UK company data fields are available, revenue and funding information access, API availability, data access methods, and how comprehensive the database is for UK private companies.",
    "task_id": "2094857",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.spitexcare.ch/ and follow the prompt: Analyze the website spitexcare.ch to understand their local SEO strategy. Look for location-specific landing pages, local keyword usage, local contact information, and local testimonials. Document your findings.",
    "task_id": "2140824",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n        **Task: Scrape Amazon Electronics (Output must be valid Python dict or list of dicts)**\n\n        **Steps:**\n        1. Open Amazon.com in the browser\n        2. Search for \"Electronics\"\n        3. Open the first item\n        4. Return the scraped details as dict, e.g.:\n           {\n             \"name\": \"Example Product\",\n             \"shop_name\": \"Example Store\",\n             \"rating\": \"4.5\",\n             \"reviews\": \"10,234\",\n             \"description\": \"...\",\n             \"url\": \"https://...\"\n           }\n        ",
    "task_id": "382924",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://derma-solution.com/ and follow the prompt: Analyze this second competitor's website homepage for complete SEO competitive analysis. Extract and document:\n\n1. Page Structure & Content:\n   - Page title, meta description, headings\n   - Content sections and organization\n   - Product positioning and categories\n   - Brand messaging and USPs\n\n2. SEO Approach:\n   - Keyword strategy and targeting\n   - Technical SEO elements\n   - Content optimization\n   - Site architecture\n\n3. Competitive Positioning:\n   - How they differentiate from other Korean beauty suppliers\n   - Trust and credibility elements\n   - Unique value propositions\n\n4. SEO Benchmarking:\n   - Strengths that upkeepskin.com should consider\n   - Weaknesses that present opportunities\n   - Best practices to adopt\n\nThis will complete our competitive analysis for comprehensive SEO recommendations.",
    "task_id": "2216354",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://medium.com/@nambos3rd/tesla-full-year-2024-analysis-a-review-of-actual-performance-my-financial-forecast-41ee70091b5a and follow the prompt: Extract comprehensive Tesla 2024 full year financial analysis including revenue trends, profit margins over time, cash flow analysis, balance sheet highlights, and key financial ratios. Focus on historical data and trends over multiple years.",
    "task_id": "1785321",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Infer using tool [infer_from_page_markdown_with_navigational_links]- extract articles (with corresponding navigational URL) that mentions opening of international luxury retail brands in Singapore. \n            Do this for subsequent pages and only consider articles that appear sequentially before article:'Snake and red jewels make fortuitous symbols this Lunar New Year'.\n            Finish condition: after the page containing 'Snake and red jewels make fortuitous symbols this Lunar New Year' has been processed and relevant articles extracted.",
    "task_id": "1377055",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.mordorintelligence.com/zh-CN/industry-reports/smart-sound-and-gateway-market and follow the prompt: \\u63d0\\u53d6\\u6587\\u7ae0\\u4e2d\\u5173\\u4e8e\\u667a\\u80fd\\u58f0\\u97f3\\u4ea7\\u54c1\\u5e02\\u573a\\u89c4\\u6a21\\u3001\\u589e\\u957f\\u8d8b\\u52bf\\u3001\\u5e02\\u573a\\u5206\\u5e03\\u3001\\u7ade\\u4e89\\u683c\\u5c40\\u4ee5\\u53ca\\u672a\\u6765\\u53d1\\u5c55\\u65b9\\u5411\\u7684\\u5185\\u5bb9\\u3002\\u91cd\\u70b9\\u5173\\u6ce8\\u4e0d\\u540c\\u7c7b\\u578b\\u58f0\\u97f3\\u4ea7\\u54c1\\u7684\\u5e02\\u573a\\u8868\\u73b0\\u53ca\\u5546\\u4e1a\\u4ef7\\u503c\\u3002",
    "task_id": "1612534",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.europages.co.uk/companies/distributor/import-export%20-%20medical%20and%20surgical%20equipment.html and follow the prompt: Extract information about medical and surgical equipment distributors, focusing on those that distribute surgical disposables like gowns, drapes, and procedure trays. Collect company names, websites, email addresses (if available), and countries. Try to browse through multiple pages if available to collect as many relevant companies as possible. Focus only on European countries.",
    "task_id": "1716501",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n                    Visit https://saytool.com/product/%EC%97%90%EC%9D%B4%EC%95%A4%EB%94%94-%EC%A0%84%EC%9E%90%EC%A0%80%EC%9A%B8%EC%A4%91%EB%9F%89%EB%B9%84%EA%B5%90-hc-30kw-hc-30kw-30kg5g/59643/\n                    If there is data corresponding to the following, please create it:\n                    Product name (prod_name), Product specifications (prod_spec), Product description (prod_desc), Model name or Model ID (model_nm), Company (company)\n                        \n                    Please provide the answer in JSON format.\n                    JSON format:\n                    {\n                        \"prod_name\": \"\",\n                        \"prod_spec\": \"\",\n                        \"prod_desc\": \"\",\n                        \"model_nm\": \"\",\n                        \"company\": \"\"\n                    }\n                ",
    "task_id": "191779",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n        You are an e-commerce site analyzer. Visit https://www.closed.com/en/women/jeans and carefully analyze the page structure.\n        \n        1. FIRST, detect and handle any popups or consent notices that might interfere with analysis.\n        2. Analyze the product containers on the page. Identify which selectors contain products.\n        3. Extract details from 3-5 sample products using the best selector you found to verify.\n        4. Examine how pagination works - check for numbered pagination, load more buttons, or infinite scroll.\n        5. Test the pagination method to verify it works.\n        \n        Organize your findings in a detailed report focusing on:\n        - Any popups detected and how to close them\n        - Product container selectors that work best for extraction\n        - Product data structure (title, price, image, link patterns)\n        - Pagination mechanism and specific selectors needed to navigate through products\n        ",
    "task_id": "1062848",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Navigate to vividseats.com and search for Toronto FC vs Inter Miami CF tickets for September 27, 2025. Extract all available ticket sections with their prices, quantities if shown, and any fees. List all sections with complete pricing information.",
    "task_id": "702654",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Extract key points from the article at https://finance.yahoo.com/quote/AAPL/news/ focusing on Apple's market position, stock performance, future predictions, and any significant events or analyses.",
    "task_id": "1017491",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Go to nasdaq.com and search for the stock symbol 'Garrett Advancing Motion'. Navigate to its historical data section and download the full-year data for 2024, if available.",
    "task_id": "959652",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://embracetransitioncoaching.com/about-paula/ and follow the prompt: Extract all content from this page including headings, text about Paula, her background, experience, coaching approach, and any other relevant information that would be useful for creating a new website",
    "task_id": "2282208",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.padmapper.com/apartments/san-francisco-ca and follow the prompt: Analyze the technical structure of PadMapper for San Francisco apartments. Identify: 1) How data is loaded (server-side vs JavaScript), 2) URL patterns for filtering apartments under $2000, 3) Available data fields and formats, 4) Any visible API endpoints or data loading mechanisms, 5) Anti-scraping measures like rate limiting or captchas, 6) Map integration and data source",
    "task_id": "1822100",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "site:agrieuro.co.uk \"charcoal grill\" 20-300 EUR reviews specifications Edilmark OR Classe OR Mastercook OR Royal Food OR Famur OR Ferraboli OR Seven Italy OR Palazzetti OR Cruccolini",
    "task_id": "795094",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://aiindex.stanford.edu/report/ and follow the prompt: Extract key findings from the latest Stanford AI Index report regarding China's position in AI talent, investment, research publications, model development, and government strategy compared to the US. Focus on sections comparing US and China AI metrics.",
    "task_id": "1421359",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n        Browse a retailer's website of https://www.thiscorner.co/ and get all products the retailer carries.\n\n        For each product, capture the following details:\n        - Product name: EXACT product name as displayed on the website\n        - Product description: EXACT product description as displayed on the website (including materials, features, benefits, dimensions, etc.). \n        - Brand name: EXACT brand name as displayed on the website. \n        - Price: EXACT price as displayed on the website. \n        - Image URLs: a list of image URLs related to the product. \n\n        NAVIGATION STRATEGY:\n        - IMMEDIATELY look for navigation elements with these high-value keywords: \"Shop\", \"Products\", \"Catalog\", \"Collections\", \"Brands\"\n        - Be methodical in checking \"Brands\" pages, navigation menus, filters, product pages, footers      \n        - Do not summarize or condense product listings - include each individual product you find\n        - If there are too many products (e.g., over 500), try to sample the products from different categories, collections, or types\n        - Prioritize variety over quantity to represent the full range of offerings\n        - If there are too many product pages, try to sample the products from different product pages and for each product page, only scrape the first screen without scrolling down            \n        - If you cannot find any products, return an empty list.\n        ",
    "task_id": "1246927",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Go to https://wro.westchesterclerk.com/landsearch.aspx, Look for land records for the Name Seven Springs LLC. For each of the resulting records, collect all details. Return a valid JSON of all records",
    "task_id": "759493",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n            Find the website for Otsego County Clerk's Office or use Google to search for the Clapper Construction LLC on Otsego County Clerk's Office (this method works best with non goverment websites such as linkedin).\n            Search for the company 'Clapper Construction LLC' with context: {\"location\": \"211 Main St NY Otego 13825 US\"} using the method: Contact the Otsego County Clerk's Office (or check their online records, if available) and search for filings under \"Clapper Construction LLC\" or the names of known principals associated with the company.  This may reveal property ownership or other business dealings..\n            Extract the following attributes: property records, business certificates, mortgages, legal filings related to the company or its principals.\n            Export all found information in structured JSON format.\n            You must include the URL you extracted the data from in the results.\n            Do not try to login to any website. You do not have credentials for any login.  \n            If at any point you get blocked by cloudflare, attempt to bypass it.  If you are unable to do so STOP.",
    "task_id": "1290796",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "{\"instructions\":{\"data_to_collect\":{\"property_info\":[\"property_name\",\"management_email\",\"phone_number\"],\"reviews\":{\"count\":2,\"details\":[\"comment_text\",\"rating\",\"date\"]},\"unit_details\":[\"available_units\",\"price_range\",\"bedroom_count\",\"bathroom_count\",\"square_footage\"]},\"sample_count\":2,\"url\":\"https://www.apartments.com/apartments/arlington-tx/min-1-bedrooms/\"},\"output_format\":{\"structure\":{\"properties\":[{\"contact\":{\"email\":\"string\",\"phone\":\"string (XXX-XXX-XXXX)\"},\"name\":\"string\",\"reviews\":[{\"date\":\"string (MM/DD/YYYY)\",\"rating\":\"number\",\"text\":\"string\"}],\"units\":[{\"availability\":\"number\",\"bathrooms\":\"number\",\"bedrooms\":\"number\",\"price\":\"number\",\"square_feet\":\"number\"}]}]},\"type\":\"json\"}}",
    "task_id": "251101",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://bstock.com/amazon/ and follow the prompt: Navigate to the B-Stock website and find the Amazon Liquidation Auctions. I am interested in auctions for 'Toys & Games' and 'Health & Beauty' pallets. Please extract information on any currently available pallets in these categories, including the product descriptions, estimated quantity, current bid, and a link to the auction.",
    "task_id": "2214895",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Goal: Find detailed information about libfreetype6 version in Ubuntu 22.04 and comprehensive details about CVE-2025-27363\nBackground motivation: Need more specific details about when the vulnerability was fixed in Ubuntu 22.04's libfreetype6 package and the nature of the vulnerability\nExpected output format: Detailed version history of libfreetype6 in Ubuntu 22.04, when the fix for CVE-2025-27363 was applied, and technical details about the vulnerability\n\nCandidate URLs: \n- https://packages.ubuntu.com/jammy/libfreetype6\n- https://ubuntu.com/security/CVE-2025-27363\n- https://launchpad.net/ubuntu/+source/freetype\n- https://people.canonical.com/~ubuntu-security/cve/2025/CVE-2025-27363.html",
    "task_id": "1624560",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.poweradspy.com/ and follow the prompt: Extract exact pricing plans, features, platform coverage (Meta, Google, TikTok), performance metrics available (ROI, CTR, CPC), integrations, unique features, trial offers, and data freshness. Focus on technical specifications and current pricing for 2025.",
    "task_id": "1783199",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Search for Metformin on goodrx.com:\n        1. First scroll down and enter location: 172 S Sunnyvale Avenue, Sunnyvale, CA 94086\n        2. Search for Metformin\n        3. Collect the following information:\n           - Available dosage forms and strengths\n           - Prices at different pharmacies nearby\n           - Any available coupons or discount programs\n           - Generic vs brand name options\n           - Common side effects and warnings\n           - Any other relevant information\n        4. Sort results by lowest price",
    "task_id": "265277",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\nGo to https://www.bseindia.com/corporates/ann.html.\nClick the button with selector \"#btnSubmit\".\nWait until the table of announcements appears.\n\nOnly on this page, for each annoucement in the table displayed,\n- Click on XBRL link. This will opne a new tab. Extract the following information from the tab: \n ScripCode,\n NameOfTheCompany, \n SubjectOfAnnouncement \n DescriptionOfAnnouncement\n AttachmentURL \n DateAndTimeOfSubmission \n CategoryOfAnnouncement\n TypeOfAnnouncement\n\nImportant instructions:\n- Do the above data extraction **ONLY for the first page**\n- If there is no XBRL link, you can ignore that annoucement\n\nReturn the data as a list of dictionaries like:\n[\n  {\n    \"ScripCode\": \"\",\n    \"NameOfTheCompany\": \"\",\n    \"SubjectOfAnnouncement\": \"\",\n    \"DescriptionOfAnnouncement\": \"\",\n    \"AttachmentURL\": \"\",\n    \"DateAndTimeOfSubmission\": \"\",\n    \"CategoryOfAnnouncement\": \"\",\n    \"TypeOfAnnouncement\": \"\"\n  },\n  ..\n]\n",
    "task_id": "1568496",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Go to https://www.zolo.ca/ and select Price and write 1600000. Then find homes with 4 bedrooms then search for homes near scott road 80 ave. Then create a json file of their addresses, price, bedrooms",
    "task_id": "246025",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "visit: https://www.uniquemallorca.com/ and search for all properites in all areas",
    "task_id": "1015772",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "go to https://www.narpm.org/find/property-managers/ and for each of the property manager cards, click VIEW PROFILE then give me the phone number on that popup. then close that popup. go to the next one on the right of it. When there are no more on the right of it, go down a level and continue until the page is done. Do not paginate.",
    "task_id": "1618713",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n    Explore the McGrath real estate website (https://www.mcgrath.com.au) and gather the following information:\n    1. Find the search input field for postcodes and its CSS selector\n    2. Find the search button and its CSS selector\n    3. Navigate to search results for postcode 2095\n    4. For property listings, identify:\n       - Container element selector\n       - Price element selector\n       - Bedrooms element selector\n       - Bathrooms element selector\n       - Car spaces element selector\n       - Property URL element selector\n    5. Identify pagination mechanism and its selector\n    6. Determine optimal wait times for each element\n    \n    Return the findings in this exact JSON format:\n    {\n        \"base_url\": \"https://www.mcgrath.com.au\",\n        \"selectors\": {\n            \"search_input\": {\"selector\": \"css_selector\", \"type\": \"css\", \"wait_time\": number},\n            \"search_button\": {\"selector\": \"css_selector\", \"type\": \"css\", \"wait_time\": number},\n            \"listing_container\": {\"selector\": \"css_selector\", \"type\": \"css\", \"wait_time\": number},\n            \"price\": {\"selector\": \"css_selector\", \"type\": \"css\", \"wait_time\": number},\n            \"bedrooms\": {\"selector\": \"css_selector\", \"type\": \"css\", \"wait_time\": number},\n            \"bathrooms\": {\"selector\": \"css_selector\", \"type\": \"css\", \"wait_time\": number},\n            \"carspaces\": {\"selector\": \"css_selector\", \"type\": \"css\", \"wait_time\": number},\n            \"listing_url\": {\"selector\": \"css_selector\", \"type\": \"css\", \"wait_time\": number}\n        },\n        \"pagination\": {\n            \"type\": \"load_more\",\n            \"selector\": \"css_selector\",\n            \"wait_time\": number\n        },\n        \"navigation\": {\n            \"search_url\": \"discovered_url_here\",\n            \"postcode_param\": \"discovered_param_here\"\n        }\n    }\n    ",
    "task_id": "1762101",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "go to www.canon.de and get all product information for system cameras. add these to a table \n\n",
    "task_id": "1130803",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Goal: Find detailed information about the SRP authentication flow and cryptographic operations in the alexrudd/cognito-srp library\nBackground motivation: Need to understand the specific implementation details of the SRP protocol, including the authentication flow steps, cryptographic operations performed, and security aspects of the library\nExpected output format: Technical details about the SRP implementation, code examples showing the authentication flow, cryptographic calculations, and security considerations\n\nCandidate URLs: \n- https://github.com/alexrudd/cognito-srp\n- https://pkg.go.dev/github.com/alexrudd/cognito-srp",
    "task_id": "1297694",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "wisconsinsurplus.com visit the site and extract the auction item details including prices, current bids, next minimum bid amounts, and images",
    "task_id": "1007520",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Browser agent: Extract district heatmap of rental yields from Bayut MarketWatch at https://www.bayut.com/mybayut/dubai-property-market-report/, return in markdown format",
    "task_id": "1821571",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n            Find the website for Uniform Commercial Code (UCC) Filings - Ohio Secretary of State or use Google to search for the All Side Roofing and Restoration LLC on Uniform Commercial Code (UCC) Filings - Ohio Secretary of State (this method works best with non goverment websites such as linkedin).\n            Search for the company 'All Side Roofing and Restoration LLC' with context: {\"location\": \"226 E 6th St, Dayton OH, 45402 USA\"} using the method: Search by company name 'All Side Roofing and Restoration LLC' to identify any UCC filings, which could indicate lenders or secured creditors..\n            Extract the following attributes: secured party, debtor, collateral.\n            Export all found information in structured JSON format.\n            You must include the URL you extracted the data from in the results.\n            Do not try to login to any website. You do not have credentials for any login.  \n            If at any point you get blocked by cloudflare, attempt to bypass it.  If you are unable to do so STOP.",
    "task_id": "1244725",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n            Find the website for Kentucky Court of Justice or use Google to search for the Amigos Roofing & Construction on Kentucky Court of Justice (this method works best with non goverment websites such as linkedin).\n            Search for the company 'Amigos Roofing & Construction' with context: {\"location\": \"4173 Willow Ln, Lexington KY 40516\"} using the method: Search the Kentucky Court of Justice online records for civil lawsuits, liens, and judgments involving 'Amigos Roofing & Construction'..\n            Extract the following attributes: lawsuits (plaintiff or defendant), judgments, liens.\n            Export all found information in structured JSON format.\n            You must include the URL you extracted the data from in the results.\n            Do not try to login to any website. You do not have credentials for any login.  \n            If at any point you get blocked by cloudflare, attempt to bypass it.  If you are unable to do so STOP.",
    "task_id": "1251782",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "First, navigate to https://www.imf.org/en/Publications/WEO/weo-database/2024/October. Then, extract. Return relevant information about steps taken and content extracted.",
    "task_id": "1999011",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Navigate to the SEC EDGAR database and search for Tesla (TSLA) filings. Identify the most recent significant filings such as 10-K, 10-Q, or 8-K.",
    "task_id": "1476683",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://app.uniswap.org/explore/pools and follow the prompt: Navega por la p\\u00e1gina de pools de Uniswap y busca espec\\u00edficamente el pool ETH/USDC o USDC/WETH. Extrae todos los datos relevantes incluyendo: APY, rendimiento, TVL, volumen 24h, comisiones, y cualquier otra m\\u00e9trica visible. Si hay varios pools ETH/USDC con diferentes fee tiers, captura los datos de todos. Tambi\\u00e9n captura informaci\\u00f3n de otros pools populares como ETH/USDT, WBTC/USDC para comparaci\\u00f3n.",
    "task_id": "2003613",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://karpathy.ai/ and follow the prompt: Analyze the page for keyword usage and content optimization. Identify:\n1. Primary keywords used on the page\n2. Keyword density and placement (in titles, headings, body)\n3. Content relevance to likely search queries\n4. Content length and depth\n5. Content organization for readability\n6. Any keyword stuffing or over-optimization issues\n\nAlso, check how effectively the content aligns with likely search intents for someone searching for Andrej Karpathy or related AI topics.",
    "task_id": "1681243",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Goal: Find non-UAF Chrome vulnerabilities in 2024-2025 specifically related to WebGPU, WebAssembly, File System Access API, Web Storage, Web Workers, CSS Module, Web Transport\n\nCandidate URLs: \n- https://chromereleases.googleblog.com/2024/\n- https://nvd.nist.gov/vuln/search/results?form_type=Advanced&results_type=overview&search_type=all&cpe_vendor=cpe%3A%2F%3Agoogle&cpe_product=cpe%3A%2F%3Agoogle%3Achrome",
    "task_id": "1077220",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Extract the following information as JSON from this product page:\nURL: https://www.uniqlo.com/kr/ko/products/E461011-000/00?colorDisplayCode=08&sizeDisplayCode=004\n\nRequired fields:\n- `name`: Product name\n- `skus`: List of SKUs with price, stock availability, and options\n- `images`: List of image URLs\n- `currency`: Currency code\n- `detailHtml`: Full HTML content of the product description\n- `optionGroups`: List of option groups, each containing:\n  - `name`: Option group name\n  - `options`: List of options with `name` and `imageUrl` (if available, otherwise an empty string)\n\nRespond with a well-formatted JSON structure.",
    "task_id": "221599",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "crawl and scrape all product data, including handling pagination, from https://skymint.com/white-cloud/category/flower, https://skymint.com/white-cloud/category/pre-rolls/, https://skymint.com/white-cloud/category/vaporizers/, https://skymint.com/white-cloud/category/edibles/, and https://skymint.com/white-cloud/category/concentrates/",
    "task_id": "124347",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\nPhase 3: Data Collection Workflow\n\nYour task is to act as an AI agent that discovers leads for a Raw Materials Importer in the Philippines.\nFollow these steps:\n\n1. Scrape Philippine Government Portals:\n   - Target the PhilGEPS website (https://www.philgeps.gov.ph).\n   - Search for public procurement tenders \n   - Extract: Company Name, Contact Details, Address, Industry, Products\n   - If information is unavailable, do a quick research with google and try to fill in as much of the missing information. Do not hallucinate information.\n\n2. Extract from Local B2B Directories:\n   - Target the Ango PH website (https://ango.ph/) and filter by categories like \"Raw Materials\" or \"Manufacturing\".\n   - Extract: Company Name, Contact Details, Address, Industry, Products\n\n3. Data Aggregation:\n   - Combine and deduplicate the extracted data.\n\n4. Report Generation:\n   - Prepare data for Google Sheets upload with fields: Company Name, Contact Details, Address, Industry, Products\n",
    "task_id": "1012772",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Goal: Gather detailed information about the identified Rails and GraphQL Ruby authentication CVEs\nExpected output: Full details for each CVE including exact vulnerability description, affected components, severity ratings, and any remediation information\n\nCandidate URLs: \n- https://nvd.nist.gov/vuln/detail/CVE-2025-27407\n- https://nvd.nist.gov/vuln/detail/CVE-2024-47887\n- https://nvd.nist.gov/vuln/detail/CVE-2021-41275\n- https://nvd.nist.gov/vuln/detail/CVE-2021-41274\n- https://nvd.nist.gov/vuln/detail/CVE-2020-8167\n- https://nvd.nist.gov/vuln/detail/CVE-2020-8166\n- https://github.com/advisories/GHSA-q92j-grw3-h492",
    "task_id": "1195938",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Navigate to: https://www.withroam.com/zipcode/91101?bedrooms=2&down_payment=400000&mode=viewport&order=ir_low&property_types=single_family&viewport=-118.47145%3A33.55095%3A-117.35938%3A34.24766\n            Meaning: Pull the Listing URL, Address, Price, Property Type, date of listing.\n            For sale in Pasadena, CA.\n            For sale in Orange County, CA.\n            ",
    "task_id": "352644",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.car.org/marketdata/data and follow the prompt: Access the California Association of Realtors (C.A.R.) market data page (https://www.car.org/marketdata/data). Look for recent reports or data sets (2023-2025) related to California housing inventory. Extract key statistics on housing supply, months of unsold inventory, or new listings vs. demand. Download relevant PDF/XLS files if available and summarize key findings.",
    "task_id": "1706786",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://warmembracegifts.com/ and follow the prompt: Navigate through this website to find and document all individual products with their specific details including:\n- Exact product names\n- Prices \n- Product descriptions\n- Categories (T-shirts, jewelry, candles, blankets, etc.)\n- Any special features or benefits mentioned\n\nPlease explore different product categories and pages to get a comprehensive list of all available products.",
    "task_id": "2271368",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "go to https://msrc.microsoft.com/update-guide/vulnerability, sort the list by release date, only consider those from february, that have the word the \"remote\" the \"cve title\", give me their CVE number, open their details and take note of their Metrics score ( looks like CVSS something )",
    "task_id": "500831",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Visit major consulting firms' methodology pages and extract structured data:1. McKinsey Insights: Extract frameworks like 7S, MECE, Issue Trees with detailed explanations2. BCG Insights: Get Growth-Share Matrix, Experience Curve methodologies3. Bain Insights: Extract Results Delivery Framework and case study approaches4. Deloitte Insights: Digital transformation methodologies and frameworks5. PwC Strategy: Business transformation approaches and toolsFor each framework, extract: Title, Description, When to use, Step-by-step process, Expected outcomesFormat as structured JSON with clear sections for each methodology.",
    "task_id": "649583",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "visit this website 'https://www.acquistinretepa.it/opencms/opencms/scheda_altri_bandi.html?idBando=51e7896e855eabe7', extract the tender information on the website and compare it with this tender {tender name: 'CUC dei Comuni dell'Area Nolana COMUNE DI CARDITO', tender description: 'CONCESSIONE DEL SERVIZIO DI RISCOSSIONE COATTIVA DI TUTTE LE ENTRATE TRIBUTARIE E PATRIMONIALI DEL COMUNE DI CARDITO, DEL RECUPERO EVASIONE DELL'IMU E TARI E DELLA GESTIONE DEL CANONE UNICO PATRIMONIALE (CUP) COMPRESO IL SERVIZIO DI GESTIONE MATERIALE DELLE PUBBLICHE AFFISSIONI DEL COMUNE DI CARDITO'.}. No need explanation and pleasantary on your response. just answer with one word as completion: \"YES\" or \"NO\". ",
    "task_id": "1504989",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n    Go to https://www.redfin.ca/ and enter the vancouver in the input box and then press the search button. After the page loads select price\n    and enter the 750000 underneath maximum. After that select the Beds & Bath and select 1 beds.\n    After the page loads, select all filters and scroll down until keyword search appears then add Keywords are: luxury, new build, trusted developer, quiet. to the keywords section.\n    After the page loads Extract maximum 5 properties with their addresses, prices, beds. \n\n        ",
    "task_id": "1067771",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.iea.org/reports/electricity-grids-and-secure-energy-transitions/executive-summary and follow the prompt: Extract comprehensive information about global electricity grid modernization policies, investment requirements by country, and specific policy recommendations for developed and developing countries. Focus on grid infrastructure upgrades, rural electrification, and policy frameworks.",
    "task_id": "1771670",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n        Get 100 item images from https://taobao.com. \n        You act like a human being, go straight to items, that are with an image, a title and a price.\n        You only get the title and the price for me.\n        Try not to leave the page to save time.\n        Scroll down only when you've got the information of all items on the screen.\n        Never use \"Extract_Content\" function on the website! Never search!\n        ",
    "task_id": "885584",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "\n    Task: Collect comprehensive information from Visa's latest earnings for analyst report.\n    \n    Steps:\n    1. Go to https://www.google.com\n    2. Search for \"Visa Q4 2024 earnings results investor relations\"\n    3. Access the official IR website and locate:\n       - Latest quarterly earnings release\n       - Earnings presentation\n       - Earnings call transcript\n       - Financial supplements\n    \n    4. For each document, extract and collect:\n       - Key financial metrics:\n         * Revenue and growth\n         * Net income and margins\n         * EPS and growth\n         * Operating metrics\n         * Cash flow and balance sheet highlights\n       - Business segment performance\n       - Management commentary on:\n         * Strategic initiatives\n         * Market conditions\n         * Future outlook\n         * Key growth drivers\n    \n    5. Format the collected information as:\n       Report URLs: [url1, url2, url3]\n       \n       Financial Highlights:\n       - Revenue: $X.XX billion (+Y% YoY)\n       - Net Income: $X.XX billion\n       - EPS: $X.XX\n       [Additional metrics...]\n       \n       Business Highlights:\n       - Segment 1: [Performance details]\n       - Segment 2: [Performance details]\n       [Additional segments...]\n       \n       Management Outlook:\n       - [Key points from management commentary]\n       - [Future guidance]\n       - [Strategic initiatives]\n    \n    Important:\n    - Focus on collecting comprehensive data for analyst report\n    - Extract both quantitative and qualitative information\n    - Include YoY comparisons where available\n    - Capture management's forward-looking statements\n    - Save detailed information to analysis_results/visa_latest_reports_raw.txt\n    ",
    "task_id": "205360",
    "category": "Direct Web Scraping"
  },
  {
    "confirmed_task": "Read webpage https://www.semrush.com/website/ and follow the prompt: Go to the Semrush website traffic analysis tool. Enter the domain 'thedefiant.io' and extract the estimated monthly traffic, top organic keywords, and a list of their main organic competitors.",
    "task_id": "2136340",
    "category": "Direct Web Scraping"
  }
]
